# OncoGraph Agent - Fine-Tuning Plan

This document outlines the dataset generation strategy, training workflow, and evaluation process for fine-tuning a bespoke Text-to-Cypher model for the OncoGraph Agent.

**Model:** `unsloth/Qwen3-4B-Instruct-2507` (4-bit quantized Qwen3-4B-Instruct optimized for Unsloth)  
**Framework:** Unsloth for efficient, memory-optimized fine-tuning with LoRA  
**Output:** Fine-tuned model available at `ib565/qwen3-4b-ft-oncograph-16bit` on Hugging Face. Lora adapters are available `ib565/qwen3-4b-ft-oncograph-lora-adapters`.

---

## Part 1: Dataset Generation Strategy

### Overview

The dataset is generated using a hybrid approach that combines systematic, template-based generation for broad coverage with data augmentation techniques to ensure linguistic diversity and robustness.

### Generation Process

1. **Systematic Generation via Templating:**
   - Python-based template generators for comprehensive query "families" (F1-F6)
   - Templates populated with real entity data (genes, diseases, variants) sampled from CIViC data files
   - Leverages canonical entity names and their ingested synonyms (especially for diseases)
   - Each question paired with a "gold" Cypher query generated by deterministic, rule-based functions

2. **Paraphrasing and Finalization:**
   - Pre-defined paraphrase templates applied to create linguistic variations
   - Final dataset split into `train_sample.jsonl` and `test_sample.jsonl` via stratified sampling by template_id

### Query Families

The dataset covers six query families, from simple lookups to complex multi-hop reasoning:

#### F1: Basic Entity Lookups (1-hop)
- **F1.1:** Therapies targeting a specific Gene
- **F1.2:** Genes targeted by a specific Therapy
- **F1.3:** The Gene a specific Variant belongs to
- **F1.4:** All Variants of a specific Gene

#### F2: Property-Based & Evidential Queries
- **F2.1:** TARGETS relationship properties (e.g., mechanism of action)
- **F2.2:** AFFECTS_RESPONSE_TO effects (sensitivity/resistance)
- **F2.3:** Evidential properties (e.g., PMIDs)
- **F2.4:** Node properties (e.g., variant consequence)

#### F3: Set-Based & Comparative Queries
- **F3.1:** Union (therapies targeting Gene A OR Gene B)
- **F3.2:** Intersection (therapies targeting Gene A AND Gene B)
- **F3.3:** Negative/Subtractive (therapies targeting Gene A but NOT Gene B)

#### F4: Multi-Hop & Validation Queries
- **F4.1:** Target validation (therapies TARGET Gene AND have AFFECTS_RESPONSE_TO evidence in Disease)
- **F4.2:** Alternative therapy discovery (resistance biomarkers → alternative therapies)

#### F5: Disease-Centric & Discovery Queries
- **F5.1:** Biomarkers with evidence in a specific Disease
- **F5.2:** Therapies with biomarker evidence in a specific Disease

#### F6: Compositional Queries
- **F6.1-F6.4:** Complex combinations of patterns from F1-F5

### Key Generation Rules

#### Disease Name Filtering (The "Cancer" Rule)

A deterministic rule handles broad vs. specific disease queries:
- If disease name contains "cancer" and has other words → remove "cancer" token (high-recall search)
- Otherwise → use all tokens (high-precision search)

**Examples:**
- `"Lung Cancer"` → uses `['lung']` → high-recall
- `"Lung Carcinoma"` → uses `['lung', 'carcinoma']` → high-precision
- `"Cancer"` → uses `['cancer']` → high-precision

#### Canonical Rule Coverage

The dataset explicitly teaches every canonical behavior:
- Gene-only AFFECTS: collapse to gene level, de-duplicate, aggregate pmids
- Variant-specific AFFECTS: prefer equality on full variant name, guarded fallbacks for tokens
- Fusions: match both orientations (e.g., `EML4::ALK` and `ALK::EML4`)
- Alteration classes: handle tokens like `Amplification`, `Overexpression`, `Deletion`, etc.
- TARGETS: project `r.moa`, derive `pmids` from reference sources
- Therapy resolution: match by exact name, synonyms, or therapy class
- Disease filters: apply "Cancer Rule" for umbrella terms vs. specific diseases
- Filter scoping: attach WHERE filters to correct bindings
- Arrays: wrap with `coalesce(..., [])` before `any()/all()`
- Effects: case-insensitive comparison
- Returns: include `therapy_name` and at least one of `variant_name` or `gene_symbol`, use `LIMIT`, no parameters

### Dataset Location

- **Raw generated pairs:** `finetuning/data/raw/generated_pairs.*.jsonl`
- **Training split:** `finetuning/data/processed/splits/train_sample.jsonl`
- **Test split:** `finetuning/data/processed/splits/test_sample.jsonl`

---

## Part 2: Training Workflow

### Directory Structure

```
finetuning/
├── data/
│   ├── raw/                          # Original generated datasets
│   └── processed/splits/             # Training/test splits
├── evaluation/
│   ├── __init__.py                   # Module exports
│   ├── harness.py                    # Model-agnostic evaluation harness
│   ├── model_adapters.py             # Protocol-based model adapters
│   └── results/                      # Evaluation checkpoints and summaries
├── notebooks/
│   ├── evaluate_baselines.ipynb   # Baseline and fine-tuned model evaluation
│   ├── compare_evaluation_results.ipynb  # Results comparison and visualization
│   └── train_qwen3_colab.ipynb   # Unsloth fine-tuning notebook
└── requirements-finetune.txt         # Python dependencies
```

### Training Configuration

**Model:** `unsloth/Qwen3-4B-Instruct-2507`  
**Method:** LoRA (Low-Rank Adaptation) with 4-bit quantization  
**Training Hyperparameters:**
- LoRA rank (`r`): 16
- LoRA alpha: 32
- LoRA dropout: 0
- Batch size: 3 per device
- Gradient accumulation: 4 steps (effective batch size: 12)
- Learning rate: 2e-4
- Warmup steps: 5
- Training epochs: 1
- Max sequence length: 1024
- Optimizer: adamw_8bit
- Weight decay: 0.001
- LR scheduler: linear

### Training Process

**Notebook:** `finetuning/notebooks/04_train_qwen3_colab.ipynb`

1. **Environment Setup:**
   - Install Unsloth and dependencies (pinned versions for Qwen3 compatibility)
   - Configure GPU runtime (designed for Google Colab T4)

2. **Data Preparation:**
   - Load `train_sample.jsonl` from `finetuning/data/processed/splits/`
   - Format pairs into Qwen3 chat template with system prompt:
     ```
     <|im_start|>system
     You are an expert Cypher query translator for oncology data.
     [schema and rules]
     <|im_end|>
     <|im_start|>user
     {question}
     <|im_end|>
     <|im_start|>assistant
     {cypher}
     <|im_end|>
     ```

3. **Model Loading:**
   - Load 4-bit quantized model via Unsloth's `FastLanguageModel`
   - Configure LoRA adapters using `get_peft_model()`

4. **Training:**
   - Use Hugging Face `SFTTrainer` with `TrainingArguments`
   - Loss computed only on assistant responses (Cypher queries)
   - Checkpointing to `finetuning/models/checkpoints/` (or Google Drive in Colab)
   - Supports resume from checkpoint if interrupted

5. **Model Saving:**
   - Save LoRA adapters to local directory: `lora_oncograph_qwen3_4b`
   - Push to Hugging Face: `ib565/qwen3-4b-ft-oncograph-16bit`
   - Optional: Save merged 16-bit model and lora adapters

### Training Output

- **LoRA adapters:** `lora_oncograph_qwen3_4b/` (local) or `ib565/qwen3-4b-ft-oncograph-lora-adapters` (Hugging Face)
- **Checkpoints:** Saved periodically during training for resumption

---

## Part 3: Evaluation System

### Architecture

The evaluation system uses a **model-agnostic design** based on Protocol-based adapters (`finetuning/evaluation/model_adapters.py`) and a unified evaluation harness (`finetuning/evaluation/harness.py`). This allows evaluating multiple models with a single codebase and enables easy addition of new models.

### Evaluation Metrics

Three levels of validation:

1. **Syntactic Validity:** Passes `RuleBasedValidator`
2. **Execution Success:** Runs on Neo4j without error
3. **Semantic Accuracy:** Returns exact same result set as gold Cypher query (order-independent, ignores extra NULL columns)

Additional metrics:
- **Latency (ms):** Generation time only
- **Token usage:** Input and output tokens
- **Tokens per second:** Generation throughput

### Evaluation Process

**Notebook:** `finetuning/notebooks/02_evaluate_baselines.ipynb`

This notebook evaluates both baseline and fine-tuned models using the same harness.

1. **Setup:**
   - Configure paths and environment variables (Neo4j connection, API keys)
   - Import evaluation harness: `Evaluator`, `run_evaluation`, and model adapters

2. **Model Configuration:**
   - Set `MODELS_TO_RUN` list (e.g., `["gemini-2.0-flash", "gemini-2.5-flash-lite", "qwen3-4b-it-2507-base", "qwen3-4b-it-2507-trained"]`)
   - Set `TEST_SUBSET_SIZE` (default: 80 for rate limit compliance)

3. **Evaluation Loop:**
   - For each model:
     - Create appropriate adapter (`GeminiModelAdapter` or `QwenModelAdapter`)
     - Load test set from `finetuning/data/processed/splits/test_sample.jsonl`
     - Create model-specific checkpoint file: `{model_id}_checkpoint.jsonl`
     - Call `run_evaluation()` which handles:
       - Checkpointing (supports resuming interrupted runs)
       - Progress tracking
       - Error handling
       - Metric computation

4. **Results:**
   - Checkpoint files: `finetuning/evaluation/results/{model_id}_checkpoint.jsonl`
   - Summary CSVs: `finetuning/evaluation/results/{model_id}_summary.csv`
   - Aggregated results: `finetuning/evaluation/results/{model_id}_evaluation_results.json`
   - Partial match analysis: `finetuning/evaluation/results/partial_matches_{model_id}.jsonl`

### Model Adapters

#### GeminiModelAdapter
- Uses 2-step pipeline: Instruction Expansion → Cypher Generation
- Rate limiting: 15 requests/minute (configurable)
- Retry logic with exponential backoff and API RetryInfo support
- Token counting via tiktoken (`o200k_base` encoding)

#### QwenModelAdapter
- Single-step direct generation
- Supports base model (`unsloth/Qwen3-4B-Instruct-2507`) and fine-tuned adapters
- Inference optimizations via Unsloth's `FastLanguageModel.for_inference()`
- Qwen3 chat template with system prompt
- Token counting via Qwen tokenizer

### Results Comparison

**Notebook:** `finetuning/notebooks/03_compare_evaluation_results.ipynb`

- Loads all evaluation result JSON files from `finetuning/evaluation/results/`
- Generates comparison tables and visualizations
- Compares metrics across all evaluated models

### Evaluated Models

- **gemini-2.0-flash:** Baseline Gemini 2.0 Flash (2-step pipeline)
- **gemini-2.5-flash-lite:** Baseline Gemini 2.5 Flash Lite (2-step pipeline)
- **qwen3-4b-it-2507-base:** Base Qwen3-4B-Instruct (untuned)
- **qwen3-4b-it-2507-trained:** Fine-tuned Qwen3-4B-Instruct with LoRA adapters

---

## Part 4: Environment Setup

### Requirements

Install dependencies from `finetuning/requirements-finetune.txt`:
- `unsloth` (with pinned versions for Qwen3 compatibility)
- `transformers[torch]`
- `peft`
- `trl`
- `bitsandbytes`
- `scikit-learn` (for stratified sampling)
- `tiktoken` (for token counting)

### Google Colab Workflow

1. Open Colab and upload or clone the repository
2. Mount Google Drive (if saving outputs to Drive)
3. Install dependencies (first cell handles this)
4. Configure paths (point to cloned repository or Drive)
5. Execute notebook cells sequentially

### Local Jupyter Workflow

1. Install Jupyter: `pip install jupyter notebook`
2. Navigate to project root
3. Launch Jupyter: `jupyter notebook finetuning/notebooks/`
4. Configure paths in notebook (point to local `finetuning/` directory)
5. Execute cells as normal

**Note:** Path configuration is the primary difference between Colab and local execution. Notebooks use configuration cells at the top where paths can be easily adjusted.

---

## Part 5: Key Design Decisions

### Notebook-Based Approach

Notebooks are the primary interface because they:
- Work seamlessly in Colab (GPU access, no code changes)
- Enable interactive debugging and visualization
- Support easy experimentation (modify hyperparameters and re-run)
- Simplify checkpoint management (resume from checkpoint with one cell change)
- Match Unsloth's example patterns

### Model-Agnostic Evaluation

The Protocol-based adapter design allows:
- Evaluating multiple models (Gemini, Qwen, future models) with the same harness
- Easy addition of new models (implement `ModelAdapter` protocol)
- Consistent metric computation across all models
- Checkpointing and resumption for all models

### Deterministic Dataset Generation

- Gold Cypher queries are generated by deterministic rules, not LLMs
- Ensures correctness and reproducibility
- Model learns mapping from natural language to correct Cypher patterns
- Disease name handling via simple, testable rules (the "Cancer Rule")

