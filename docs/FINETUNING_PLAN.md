# OncoGraph Agent - Fine-Tuning Plan

This document outlines the strategy and steps for fine-tuning a bespoke language model for the Text-to-Cypher generation task in the OncoGraph Agent.

## 1. Overview & Goal

The current system uses a two-step LLM chain (Instruction Expansion → Cypher Generation) with a general-purpose model (Gemini). While effective, this approach can be improved in terms of latency, cost, and accuracy for our specific domain.

**Goal:** To replace the current system with a single, fine-tuned model that directly translates a user's natural language question into a safe, executable Cypher query. This will serve as a high-impact portfolio piece demonstrating end-to-end AI engineering capabilities.

## 2. Dataset Strategy: Hybrid Generation & Curation

We will create a high-quality dataset of `(question, cypher)` pairs using a hybrid approach that combines systematic generation for coverage and manual curation for realism.

### Step 1: Systematic Generation
- **Method:** We will write Python-based template generators for a comprehensive set of query "families" (see below). These templates will be populated with real entity data (genes, diseases, etc.) sampled from the project's existing CSV files. The generation script will leverage canonical entity names and their ingested synonyms (especially for diseases) to create a rich training set. For each canonical disease, the script will generate multiple varied questions using both the canonical name and its aliases (e.g., "non-small cell lung cancer"). Each question will be paired with a "gold" Cypher query generated by a deterministic, rule-based function. This process teaches the model to map diverse user phrasing to a single, robust, and correctly scoped query.
- **Output:** ~1000-2000 `(question, gold_cypher)` pairs. The Cypher is considered "gold" because it's generated from deterministic, correct templates, not an LLM.

### Step 2: Question Curation and Dataset Finalization
- **Method:** After generating the `(question, gold_cypher)` pairs, the list of questions will be exported for manual review. This ensures linguistic quality and diversity.
- **Task:** The human curator will review and edit the list of generated questions to improve their naturalness and realism. The corresponding "gold" Cypher for each question remains unchanged.
- **Output:** A final, curated set of pairs, which will then be split into `train.jsonl` and `test.jsonl`.

## 3. Query Families

To ensure comprehensive coverage of the graph's capabilities, the dataset will be generated based on the following query patterns. The families are structured from simple lookups to complex, multi-hop reasoning.

#### F1: Basic Entity Lookups (1-hop)
- **F1.1 (Therapy -> Gene):** Therapies targeting a specific `Gene`.
  - *Example:* "What drugs target BRAF?"
- **F1.2 (Gene -> Therapy):** Genes targeted by a specific `Therapy` (Symmetric pattern).
  - *Example:* "Which genes does Dabrafenib target?"
- **F1.3 (Variant -> Gene):** The `Gene` a specific `Variant` belongs to.
  - *Example:* "Which gene is BRAF V600E a variant of?"
- **F1.4 (Gene -> Variants):** All `Variants` of a specific `Gene`.
  - *Example:* "List all known variants of the KRAS gene."

#### F2: Property-Based & Evidential Queries
- **F2.1 (TARGETS Properties):** Requesting properties from the `TARGETS` relationship.
  - *Example:* "What is Dabrafenib's mechanism of action on BRAF?"
- **F2.2 (AFFECTS_RESPONSE_TO Basic):** The effect of a `Biomarker` on a `Therapy`, often constrained by a `Disease`.
  - *Example:* "How does EGFR T790M affect response to Osimertinib in lung cancer?"
- **F2.3 (Evidential Properties):** Requesting specific evidence details from the `AFFECTS_RESPONSE_TO` relationship.
  - *Example:* "Provide PMIDs supporting that EGFR S492R confers resistance to cetuximab."
- **F2.4 (Node Properties):** Requesting properties from a node itself.
    - *Example:* "What is the consequence of the BRAF V600E variant?"

#### F3: Set-Based & Comparative Queries
- **F3.1 (Union):** Therapies targeting `Gene A` OR `Gene B`.
  - *Example:* "Find therapies for EGFR or ERBB2."
- **F3.2 (Intersection):** Therapies targeting `Gene A` AND `Gene B`.
  - *Example:* "What therapies target both BRAF and MEK1?"
- **F3.3 (Negative / Subtractive):** Therapies targeting `Gene A` but NOT `Gene B`.
  - *Example:* "Which drugs target KRAS but not NRAS?"

#### F4: Multi-Hop & Validation Queries (High Value)
- **F4.1 (Target Validation):** Find therapies that `TARGET` a specific `Gene` AND have known `AFFECTS_RESPONSE_TO` biomarker evidence in a `Disease`.
  - *Example:* "Which therapies target BRAF and also have biomarker evidence in melanoma?"
- **F4.2 (Alternative Therapy Discovery):** Find `Genes` that are resistance biomarkers for `Therapy A`, then find other therapies (`Therapy B`) that `TARGET` those `Genes`.
  - *Example:* "For therapies causing resistance via KRAS mutations, what are some alternative drugs targeting KRAS?"

#### F5: Disease-Centric & Discovery Queries
- **F5.1 (Disease -> Biomarkers):** Find all biomarkers (Genes or Variants) with evidence in a specific `Disease`.
  - *Example:* "In colorectal cancer, which ERBB2 or EGFR variants have biomarker evidence?"
- **F5.2 (Disease -> Therapies):** Find all therapies that have known biomarker evidence in a specific `Disease`.
  - *Example:* "List therapies with variant-level biomarkers in non-small cell lung cancer."

#### F6: Compositional Queries (High Value)
To ensure the model learns to handle complex, real-world questions, a dedicated family of compositional queries will be included. These queries explicitly combine patterns from the simpler families to help the model generalize.

- **F6.1 (Evidence + Therapy Union + Disease Filter):** A combination of `F2.2`, `F3.1`, and `F5`.
  - *Example:* "Which genes predict resistance to cetuximab or panitumumab in colorectal cancer?"
- **F6.2 (Targeting + Negative Filter + Disease Evidence):** A combination of `F1.1`, `F3.3`, and `F4.1`.
  - *Example:* "What therapies target BRAF but NOT KRAS, and have biomarker evidence in melanoma?"
- **F6.3 (Targeting + Variant Evidence):** A combination of `F1.1`, `F2.2`, and `F5`.
  - *Example:* "Find therapies that target EGFR for variants that cause resistance in lung cancer."
 - **F6.4 (Alternative Therapy via Resistance Genes + Disease):** A combination of `F4.2` and `F5`.
   - *Example:* "For resistance to cetuximab in colorectal cancer, which therapies target the resistance biomarkers?"

## 4. Key Generation Strategies

To ensure the model is robust to variations in user phrasing, the dataset will be designed to teach the following specific Cypher generation patterns:

### Disease Name Filtering: A Deterministic Approach

To provide an intuitive search experience for researchers, the system must handle both broad (e.g., "lung cancer") and specific (e.g., "lung adenocarcinoma") queries correctly. The intelligence for this is implemented as a simple, deterministic rule in the `generate_dataset.py` script, not inferred by the LLM.

- **The LLM's Task:** The fine-tuned model's only job is to learn to map a user's phrasing to the most appropriate canonical disease name from the database. It is a simple "translation" task, trained using the disease names and their ingested synonyms.

- **The Dataset Generator's Logic (The "Cancer" Rule):** The script that generates the "gold" Cypher query for each training example will apply the following rule:
  > When tokenizing a canonical disease name, if the word `"cancer"` is present **and** it is not the only word, remove it from the list of tokens used to build the query.

- **Behavior:** This single rule produces the desired high-recall/high-precision behavior:
  - `"Lung Cancer"` (tokens `{'lung', 'cancer'}`) → rule applies → query uses `['lung']` → **High-Recall Search**.
  - `"Lung Carcinoma"` (tokens `{'lung', 'carcinoma'}`) → rule does not apply → query uses `['lung', 'carcinoma']` → **High-Precision Search**.
  - `"Cancer"` (token `{'cancer'}`) → rule does not apply → query uses `['cancer']` → **High-Precision Search**.

- **Example High-Recall Gold Cypher (for "Lung Cancer"):**
  ```cypher
  ...
  WHERE toLower(rel.disease_name) CONTAINS toLower('lung')
  ...
  ```

- **Example High-Precision Gold Cypher (for "Lung Carcinoma"):**
  ```cypher
  ...
  WHERE toLower(rel.disease_name) CONTAINS toLower('lung')
    AND toLower(rel.disease_name) CONTAINS toLower('carcinoma')
  ...
  ```
This deterministic approach ensures that the logic is simple, testable, and robust, giving the LLM a clean and achievable learning task.

### Canonical Rule Coverage (Alignment with prompts.py)

The dataset must explicitly teach every canonical behavior encoded in `src/pipeline/prompts.py`.

- Rules to teach (non‑exhaustive but mandatory):
  - Gene‑only AFFECTS: collapse evidence to gene level; set `variant_name = NULL`; de‑duplicate by `(gene_symbol, therapy_name, disease_name)`; aggregate `pmids` across rows; return `pmids` as array (default `[]`).
  - Variant‑specific AFFECTS: prefer equality on full variant name; if only a token (e.g., `G12C`), use guarded fallbacks: `Variant.name CONTAINS`, or `hgvs_p = 'p.<TOKEN>'`, or `synonyms CONTAINS`; always guard with `[:VARIANT_OF]` to the gene.
  - Fusions: match both orientations (e.g., `EML4::ALK` and `ALK::EML4`).
  - Alteration classes: handle tokens like `Amplification`, `Overexpression`, `Deletion`, `Loss-of-function`, `Fusion`, `Wildtype` with `VARIANT_OF` guard.
  - TARGETS: project `r.moa AS targets_moa`; derive `pmids` from `r.ref_sources/r.ref_ids` where source contains `pubmed` or equals `pmid` (case‑insensitive); also return `ref_sources/ref_ids/ref_urls`.
  - Therapy resolution: match by exact name; synonyms equality; `toLower(t.name) CONTAINS`; therapy class via `tags` or via explicit `[:TARGETS]` to the gene.
  - Disease filters: umbrella terms use minimal anchor token(s); specific diseases use canonical tokens; apply the “Cancer Rule” from this plan; disease aliases expand question phrasing in training, not schema.
  - Filter scoping: attach WHERE filters to the bindings they constrain; do not place relationship filters under `OPTIONAL MATCH`.
  - Arrays: wrap with `coalesce(..., [])` before `any()/all()`; always return `pmids` as an array (default `[]`).
  - Effects: compare case‑insensitively (`toLower(rel.effect) = 'resistance'|'sensitivity'`).
  - Returns: include `therapy_name` and at least one of `variant_name` or `gene_symbol`; use minimal sufficient columns; set missing columns to `NULL`; include `LIMIT`; no parameters (no `$vars`).

- Family coverage map:
  - F1.1/F1.2: TARGETS lookups; therapy name/synonyms; tags‑based class; minimal returns.
  - F1.3/F1.4: VARIANT_OF guard; variant/gene traversal; alteration class tokens.
  - F2.1: MOA + reference‑derived pmids; include reference arrays.
  - F2.2: AFFECTS gene‑only and variant‑specific; sensitivity/resistance; disease filters; pmids as array.
  - F2.3: Evidence property extraction (e.g., PMIDs).
  - F2.4: Node properties (e.g., variant `consequence`).
  - F3: Union/intersection/negative on genes/therapies.
  - F4.1: Target validation combining TARGETS + AFFECTS + disease; correct filter scoping; mixed return schema.
  - F4.2: Alternative therapy via resistance genes; multi‑hop with disease.
  - F5: Disease name handling with aliases and the deterministic Cancer Rule.
  - F6: Compositional combinations of the above (generalization exemplars).

- Dataset generation requirements (must include explicit examples for):
  - Gene‑only AFFECTS with pmid aggregation and de‑duplication.
  - Variant‑specific AFFECTS: full names, bare tokens (e.g., `G12C`), `hgvs_p`, synonyms.
  - Fusion variants both orientations.
  - Alteration‑class tokens with `VARIANT_OF` guard.
  - TARGETS with reference‑derived pmids and returned reference arrays.
  - Therapy class via `tags` (e.g., “anti‑EGFR”) and fallback via TARGETS.
  - Disease broad vs specific for the same evidence (e.g., “Lung Cancer” vs “Lung Carcinoma” vs NSCLC) per Cancer Rule.
  - Set ops: union, intersection, negative (e.g., “KRAS but not NRAS”).
  - Mixed return schemas with NULL placeholders where fields are inapplicable.
  - Usage of `coalesce(..., [])` in `any()/all()` over arrays; inclusion of `LIMIT`; no parameters.

## 5. Development Workflow & Tooling

All development will occur within this mono-repo to maintain tight integration with existing schema and validation logic.

### Directory Structure
```
.
├── scripts/
│   ├── generate_dataset.py   # Systematically generates (question, cypher) pairs
│   ├── execute_queries.py    # Executes cypher and stores results for validation
│   ├── split_dataset.py      # Script to split the curated data into train/test sets
│   ├── train_model.py        # Runs the fine-tuning job
│   └── evaluate_model.py     # Runs the evaluation harness on the test set
├── dataset/
│   ├── generated_pairs.jsonl
│   ├── executed_results.jsonl
│   └── final_dataset/        # Contains train.jsonl and test.jsonl
├── src/
│   ├── pipeline/
│   │   ├── local_model.py    # New file for the fine-tuned generator component
...
```

### Process
1.  Run `generate_dataset.py` to create the initial set of pairs.
2.  Manually review and edit the generated questions for quality and naturalness.
3.  Run `split_dataset.py` to create the final `train` and `test` splits.
4.  Run `train_model.py` using the `train.jsonl` file.
5.  Run `evaluate_model.py` on `test.jsonl` to benchmark the new model.

## 6. Evaluation Plan

Model performance will be assessed using a dedicated, held-out test set (~15-20% of the curated data). The evaluation script will be separate from the application's main tracing/logging system.

### Multi-Level Metrics
The script will report on the following metrics for the fine-tuned model vs. the baseline Gemini model:

1.  **Syntactic Validity:** Percentage of generated queries that pass the `RuleBasedValidator`.
2.  **Execution Success:** Percentage of valid queries that execute on Neo4j without error.
3.  **Semantic Accuracy (Result Matching):** Percentage of executed queries that return the **exact same result set** as the "gold" Cypher query from the test set. This is the primary measure of success.
4.  **Performance:** Average latency (ms) per query.

The results will be printed in a comparison table to clearly demonstrate the fine-tuned model's performance.
