# Fine-Tuning Technical Details

Complete technical documentation for fine-tuning the OncoGraph text-to-Cypher model.

**Framework:** Unsloth with LoRA  

For model information, Hugging Face links, and evaluation results, see [FINETUNING_OVERVIEW.md](FINETUNING_OVERVIEW.md).

---

## Dataset Generation

**Strategy:** Template-based generation covering query families F1-F6, populated with real CIViC data. Each question paired with deterministic "gold" Cypher query.

**Data Augmentation:**
- **Question paraphrasing:** Multiple phrasings generated per template using paraphrase templates
- **Disease synonym mapping:** Questions use synonyms (e.g., "Non-small Cell Lung Carcinoma", "Lung Non-small Cell Cancer") while Cypher queries always reference canonical disease names, teaching the model to map user phrasing to ground truth entities
- **Deterministic ground truth:** All Cypher queries generated by rule-based functions, ensuring correctness and reproducibility

**Query Families:**
- **F1:** Basic entity lookups (therapyâ†”gene, variantâ†”gene)
- **F2:** Property-based & evidential queries (TARGETS properties, AFFECTS_RESPONSE_TO effects, PMIDs, node properties)
- **F3:** Set operations (union, intersection, negative)
- **F4:** Multi-hop validation (target validation, alternative therapy discovery)
- **F5:** Disease-centric queries (biomarkers/therapies by disease)
- **F6:** Miscellaneous complex compositional queries (novel combinations of F1-F5 patterns)

**Key Rules:**
- **Cancer Rule:** If disease name contains "cancer" + other words â†’ remove "cancer" token (high-recall). Otherwise use all tokens (high-precision).
- **Canonical coverage:** Gene-only AFFECTS (collapse/de-duplicate), variant-specific AFFECTS (equality + fallbacks), fusions (both orientations), alteration classes, TARGETS MOA/pmids, therapy resolution, disease filters, filter scoping, arrays (`coalesce`), case-insensitive effects, returns (LIMIT, no parameters).

**Location:**
- Raw: `finetuning/data/raw/generated_pairs.*.jsonl`
- Splits: `finetuning/data/processed/splits/{train,test}_sample.jsonl`

---

## Training

**Notebook:** `finetuning/notebooks/train_qwen3_colab.ipynb`

**Hyperparameters:**
- LoRA: `r=16`, `alpha=32`, `dropout=0`
- Batch: 3 per device, gradient accumulation 4 (effective 12)
- Training: `lr=2e-4`, 1 epoch, `max_seq_length=1024`, `adamw_8bit`, warmup 5 steps

**Process:**
1. Load `train_sample.jsonl` and format into Qwen3 chat template
2. Load 4-bit quantized model via Unsloth
3. Configure LoRA adapters
4. Train with `SFTTrainer` (loss on assistant responses only)
5. Save LoRA adapters and push to Hugging Face

**Output:**
- LoRA adapters: `lora_oncograph_qwen3_4b/` (local) or Hugging Face
- Merged 16-bit model: Hugging Face
- Checkpoints: `finetuning/models/checkpoints/`

**Inference Performance Notes:**
- 16-bit merged models provide faster inference than loading LoRA adapters separately and are vLLM-compatible
- Current inference bottleneck is free Colab T4 GPU (using Unsloth); vLLM may offer further optimization (untested)

---

## Evaluation

**Notebooks:**
- **Evaluation:** `finetuning/notebooks/evaluate_models.ipynb` (evaluates both baselines and fine-tuned models)
- **ðŸ“Š Model Evaluation Report:** `https://ib565.github.io/OncoGraph/model_evaluation_report.html` - Comprehensive evaluation results with detailed graphs and analysis

**Architecture:** Model-agnostic design using Protocol-based adapters (`finetuning/evaluation/model_adapters.py`) and unified harness (`finetuning/evaluation/harness.py`).

**Metrics:**
1. Syntactic validity (passes `RuleBasedValidator`)
2. Execution success (runs on Neo4j)
3. Output accuracy (exact result match with gold Cypher)
4. Latency, token usage, throughput

**Error Analysis:** Manual review of cases where syntactic and execution pass but output fails (saved to `partial_matches_{model_id}.jsonl`) to identify systematic patterns in result mismatches.

**Process:**
1. Configure `MODELS_TO_RUN` list
2. For each model: create adapter, load test set, run `run_evaluation()` with checkpointing
3. Results saved to `finetuning/evaluation/results/{model_id}_*.{jsonl,csv,json}`

**Models Evaluated:**
- `gemini-2.0-flash`, `gemini-2.5-flash-lite` (2-step pipeline)
- `qwen3-4b-it-2507-base` (untuned)
- `qwen3-4b-it-2507-trained` (fine-tuned Qwen3-4B)
- `qwen3-1.7b-trained` (fine-tuned Qwen3-1.7B)

For detailed results, see the [results summary table](FINETUNING_OVERVIEW.md#results) in the overview or the [Model Evaluation Report](https://ib565.github.io/OncoGraph/model_evaluation_report.html).

---

## Setup

**Requirements:** `finetuning/requirements-finetune.txt` (unsloth, transformers, peft, trl, bitsandbytes, scikit-learn, tiktoken)

**Workflow:** Notebooks work in both Google Colab and local Jupyter. Configure paths in notebook cells (Colab: point to cloned repo/Drive; local: point to project `finetuning/` directory).

---

## Design Decisions

**Key Design Principles:**
- **Notebook-based approach:** Seamless Colab/local execution, interactive debugging, easy experimentation
- **Model-agnostic evaluation:** Protocol-based adapters enable consistent evaluation across models
- **Deterministic dataset generation:** Rule-based Cypher generation ensures correctness and reproducibility

**Implementation Notes:**
- **Evaluation subset:** Default test subset is 80 records to comply with Gemini RPM limits; harness supports checkpointed resumption for larger runs
- **Token counting:** Gemini adapters use `tiktoken` (`o200k_base`); Qwen adapter uses the model tokenizer and records precise generated token counts for throughput

