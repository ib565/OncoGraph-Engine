# Fine-Tuning Overview

High-level overview of OncoGraph fine-tuning project for text-to-Cypher generation.

---

## Problem

OncoGraph translates natural language questions about oncology data into Neo4j Cypher queries. The original system used a two-step LLM pipeline (Gemini) that had the following limitations:
- **Lack of schema/dataset-specific knowledge**
- **High latency:** Multiple API calls per query
- **High cost:** API usage per request
- **Limited domain specificity:** General-purpose model not optimized for Cypher generation

**Idea:** Accurate cypher generation requires a lot of schema-specific and dataset-specific knowledge. Giving it all via prompt engineering or RAG is not scalable or flexible. Instead, a small model can be fine-tuned to perform well on this specific schema.

---

## Solution

Fine-tuned open source Qwen models on oncology-specific question-to-Cypher pairs to enable **single-step, direct text-to-Cypher generation**.

**Key Decisions:**
- **Deterministic ground truth:** All training Cypher queries generated by rule-based functions (not LLMs), ensuring correctness
- **Data augmentation:** Questions use disease synonyms and paraphrasing while Cypher always references canonical names, teaching synonym mapping
- **Comprehensive coverage:** Six query families (F1-F6) covering basic lookups to complex multi-hop reasoning

**Models Trained:**
- **Qwen3-1.7B:** Base model `unsloth/Qwen3-1.7B-unsloth-bnb-4bit` fine-tuned with LoRA
- **Qwen3-4B:** Base model `unsloth/qwen3-4b-instruct-2507-unsloth-bnb-4bit` fine-tuned with LoRA

**Available on Hugging Face:**
Each model is available in two formats (16-bit vLLM-compatible merged model and LoRA adapters):

**Qwen3-1.7B-Oncograph:**
- Merged 16-bit model: [`ib565/qwen3-1.7b-ft-oncograph-16bit`](https://huggingface.co/ib565/qwen3-1.7b-ft-oncograph-16bit)
- LoRA adapters: [`ib565/qwen3-1.7b-ft-oncograph-lora-adapters`](https://huggingface.co/ib565/qwen3-1.7b-ft-oncograph-lora-adapters)

**Qwen3-4B-Oncograph:**
- Merged 16-bit model: [`ib565/qwen3-4b-ft-oncograph-16bit`](https://huggingface.co/ib565/qwen3-4b-ft-oncograph-16bit)
- LoRA adapters: [`ib565/qwen3-4b-ft-oncograph-lora-adapters`](https://huggingface.co/ib565/qwen3-4b-ft-oncograph-lora-adapters)


**Deployment:** To be figured out. Looking for some budget friendly inference options.

---

## Results

**ðŸ“Š [View comprehensive evaluation results and graphs â†’](https://ib565.github.io/OncoGraph/model_evaluation_report.html)**

The evaluation report includes detailed visualizations of:
- Accuracy metrics (syntactic validity, execution success, output accuracy)
- Qwen models: before vs after fine-tuning comparison
- Latency and token usage analysis
- Accuracy vs latency trade-off analysis

### Summary Table

| Model | Syntactic Validity | Execution Success | **Output Accuracy** | Avg Latency |
|-------|-------------------|-------------------|----------------------|-------------|
| **qwen3-4b-ft-oncograph** (fine-tuned) | **100%** | **100%** | **91.25%** | ~14.0s |
| **qwen3-1.7b-ft-oncograph** (fine-tuned) | **100%** | **98.75%** | **72.5%** | ~9.9s |
| Gemini 2.0 Flash (baseline) | 98.75% | 88.75% | 48.75% | ~14.6s |
| Gemini 2.5 Flash Lite (baseline) | 100% | 92.5% | 41.25% | **~6.4s** |
| qwen3-4b-base (untuned) | 88.75% | 53.75% | 3.75% | N/A |

Test set size: 80 queries; evaluation harness supports checkpointed resumption for larger runs.

**Evaluation depth:** Three-level validation (syntactic â†’ execution â†’ output accuracy) with manual error analysis for systematic improvements

**Performance Notes:**
- Fine-tuned model latency is bottlenecked by free Colab T4 GPU; 16-bit merged models provide faster inference than LoRA adapters and are vLLM-compatible
- Qwen3-1.7B offers a good accuracy/speed trade-off with ~72.5% accuracy at ~9.9s latency
- Qwen3-4B achieves highest accuracy (91.25%) but at higher latency (~14s)
- Potential optimizations: use vLLM for inference, use a faster GPU

---

## Impact

- **2x improvement** in output accuracy over baseline models (Qwen3-4B: 91.25% vs Gemini 2.0 Flash: 48.75%)
- **~100% reliability** in generating valid, executable Cypher queries (both fine-tuned models achieve 100% syntactic validity)
- **Single-step generation** eliminates multi-call latency and API costs
- **Domain-specific expertise** trained on oncology data patterns
- **Model options:** Choose between Qwen3-1.7B (faster, 72.5% accuracy) or Qwen3-4B (higher accuracy, 91.25%) based on latency/accuracy trade-off needs

---

## Technical Details

For detailed information on dataset generation, training configuration, evaluation methodology, and key design decisions, see:
- **[FINETUNING_DETAILS.md](FINETUNING_DETAILS.md)** - Complete technical documentation
- **[Model Evaluation Report](https://ib565.github.io/OncoGraph/model_evaluation_report.html)** - Comprehensive evaluation results with visualizations and analysis

