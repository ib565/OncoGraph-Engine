{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation Results Comparison\n",
        "\n",
        "This notebook compares evaluation results across multiple models:\n",
        "- Gemini 2.5 Flash Lite\n",
        "- Qwen3-4B-Instruct-2507\n",
        "- Gemini 2.0 Flash (coming soon)\n",
        "\n",
        "Metrics compared:\n",
        "- Syntactic validity (%)\n",
        "- Execution success (%)\n",
        "- Semantic accuracy (%)\n",
        "- Average latency (ms)\n",
        "- Token usage (input/output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure plotting\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Set up paths\n",
        "NOTEBOOK_DIR = Path(os.getcwd())\n",
        "RESULTS_DIR = NOTEBOOK_DIR.parent / \"evaluation\" / \"results\"\n",
        "\n",
        "print(f\"Results directory: {RESULTS_DIR}\")\n",
        "print(f\"Results directory exists: {RESULTS_DIR.exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Discover and load all evaluation result JSON files\n",
        "def load_evaluation_results(results_dir: Path) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Load all evaluation result JSON files from the results directory.\"\"\"\n",
        "    all_results = {}\n",
        "    \n",
        "    # Find all *_evaluation_results.json files\n",
        "    result_files = list(results_dir.glob(\"*_evaluation_results.json\"))\n",
        "    \n",
        "    print(f\"Found {len(result_files)} evaluation result files:\")\n",
        "    \n",
        "    for result_file in sorted(result_files):\n",
        "        # Extract model name from filename (e.g., \"gemini-2.5-flash-lite_evaluation_results.json\" -> \"gemini-2.5-flash-lite\")\n",
        "        model_name = result_file.stem.replace(\"_evaluation_results\", \"\")\n",
        "        \n",
        "        print(f\"  Loading: {model_name}\")\n",
        "        \n",
        "        with result_file.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "            \n",
        "            # Extract metrics for this model\n",
        "            if \"models\" in data and model_name in data[\"models\"]:\n",
        "                all_results[model_name] = data[\"models\"][model_name][\"metrics\"]\n",
        "                all_results[model_name][\"timestamp\"] = data.get(\"timestamp\", \"\")\n",
        "                all_results[model_name][\"test_set_size\"] = data.get(\"test_set_size\", 0)\n",
        "            else:\n",
        "                print(f\"    Warning: Could not find metrics for {model_name}\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# Load all results\n",
        "evaluation_results = load_evaluation_results(RESULTS_DIR)\n",
        "\n",
        "print(f\"\\nLoaded {len(evaluation_results)} model evaluation results\")\n",
        "print(f\"Models: {list(evaluation_results.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "def create_comparison_dataframe(results: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
        "    \"\"\"Create a DataFrame for easy comparison of metrics across models.\"\"\"\n",
        "    \n",
        "    rows = []\n",
        "    \n",
        "    for model_name, metrics in results.items():\n",
        "        row = {\n",
        "            \"Model\": model_name,\n",
        "            \"Syntactic Validity (%)\": metrics.get(\"syntactic_validity_pct\", 0),\n",
        "            \"Execution Success (%)\": metrics.get(\"execution_success_pct\", 0),\n",
        "            \"Semantic Accuracy (%)\": metrics.get(\"semantic_accuracy_pct\", 0),\n",
        "            \"Avg Latency (ms)\": metrics.get(\"avg_latency_ms\", 0),\n",
        "            \"Avg Input Tokens\": metrics.get(\"avg_input_tokens\", 0),\n",
        "            \"Avg Output Tokens\": metrics.get(\"avg_output_tokens\", 0),\n",
        "            \"Total Input Tokens\": metrics.get(\"total_input_tokens\", 0),\n",
        "            \"Total Output Tokens\": metrics.get(\"total_output_tokens\", 0),\n",
        "            \"Test Set Size\": metrics.get(\"test_set_size\", metrics.get(\"total\", 0)),\n",
        "            \"Timestamp\": metrics.get(\"timestamp\", \"\"),\n",
        "        }\n",
        "        rows.append(row)\n",
        "    \n",
        "    df = pd.DataFrame(rows)\n",
        "    \n",
        "    # Sort by semantic accuracy (descending)\n",
        "    df = df.sort_values(\"Semantic Accuracy (%)\", ascending=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "comparison_df = create_comparison_dataframe(evaluation_results)\n",
        "print(\"\\nComparison Table:\")\n",
        "display(comparison_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format comparison table for better display (percentage metrics only)\n",
        "display_cols = [\n",
        "    \"Model\",\n",
        "    \"Syntactic Validity (%)\",\n",
        "    \"Execution Success (%)\",\n",
        "    \"Semantic Accuracy (%)\",\n",
        "    \"Avg Latency (ms)\",\n",
        "    \"Avg Input Tokens\",\n",
        "    \"Avg Output Tokens\",\n",
        "]\n",
        "\n",
        "print(\"\\nKey Metrics Comparison:\")\n",
        "display(comparison_df[display_cols].round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 1: Accuracy Metrics Comparison (Bar Chart)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Prepare data\n",
        "x = range(len(comparison_df))\n",
        "width = 0.25\n",
        "\n",
        "syntactic = comparison_df[\"Syntactic Validity (%)\"]\n",
        "execution = comparison_df[\"Execution Success (%)\"]\n",
        "semantic = comparison_df[\"Semantic Accuracy (%)\"]\n",
        "\n",
        "# Create bars\n",
        "bars1 = ax.bar([i - width for i in x], syntactic, width, label=\"Syntactic Validity\", alpha=0.8)\n",
        "bars2 = ax.bar(x, execution, width, label=\"Execution Success\", alpha=0.8)\n",
        "bars3 = ax.bar([i + width for i in x], semantic, width, label=\"Semantic Accuracy\", alpha=0.8)\n",
        "\n",
        "# Customize\n",
        "ax.set_xlabel(\"Model\", fontsize=12, fontweight=\"bold\")\n",
        "ax.set_ylabel(\"Percentage (%)\", fontsize=12, fontweight=\"bold\")\n",
        "ax.set_title(\"Accuracy Metrics Comparison Across Models\", fontsize=14, fontweight=\"bold\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(comparison_df[\"Model\"], rotation=45, ha=\"right\")\n",
        "ax.set_ylim(0, 110)\n",
        "ax.legend(loc=\"upper left\")\n",
        "ax.grid(True, alpha=0.3, axis=\"y\")\n",
        "\n",
        "# Add value labels on bars\n",
        "def add_value_labels(bars):\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(\n",
        "            bar.get_x() + bar.get_width() / 2.,\n",
        "            height,\n",
        "            f\"{height:.1f}%\",\n",
        "            ha=\"center\",\n",
        "            va=\"bottom\",\n",
        "            fontsize=9,\n",
        "        )\n",
        "\n",
        "add_value_labels(bars1)\n",
        "add_value_labels(bars2)\n",
        "add_value_labels(bars3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 2: Performance Metrics (Latency)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "bars = ax.bar(\n",
        "    comparison_df[\"Model\"],\n",
        "    comparison_df[\"Avg Latency (ms)\"],\n",
        "    alpha=0.7,\n",
        "    color=\"steelblue\",\n",
        ")\n",
        "\n",
        "ax.set_xlabel(\"Model\", fontsize=12, fontweight=\"bold\")\n",
        "ax.set_ylabel(\"Average Latency (ms)\", fontsize=12, fontweight=\"bold\")\n",
        "ax.set_title(\"Average Latency Comparison\", fontsize=14, fontweight=\"bold\")\n",
        "ax.tick_params(axis=\"x\", rotation=45)\n",
        "ax.grid(True, alpha=0.3, axis=\"y\")\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(\n",
        "        bar.get_x() + bar.get_width() / 2.,\n",
        "        height,\n",
        "        f\"{height:.0f} ms\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=10,\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 3: Token Usage Comparison\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Input tokens\n",
        "bars1 = ax1.bar(\n",
        "    comparison_df[\"Model\"],\n",
        "    comparison_df[\"Avg Input Tokens\"],\n",
        "    alpha=0.7,\n",
        "    color=\"coral\",\n",
        ")\n",
        "ax1.set_xlabel(\"Model\", fontsize=12, fontweight=\"bold\")\n",
        "ax1.set_ylabel(\"Average Input Tokens\", fontsize=12, fontweight=\"bold\")\n",
        "ax1.set_title(\"Average Input Tokens per Query\", fontsize=13, fontweight=\"bold\")\n",
        "ax1.tick_params(axis=\"x\", rotation=45)\n",
        "ax1.grid(True, alpha=0.3, axis=\"y\")\n",
        "\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(\n",
        "        bar.get_x() + bar.get_width() / 2.,\n",
        "        height,\n",
        "        f\"{height:.0f}\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=9,\n",
        "    )\n",
        "\n",
        "# Output tokens\n",
        "bars2 = ax2.bar(\n",
        "    comparison_df[\"Model\"],\n",
        "    comparison_df[\"Avg Output Tokens\"],\n",
        "    alpha=0.7,\n",
        "    color=\"lightgreen\",\n",
        ")\n",
        "ax2.set_xlabel(\"Model\", fontsize=12, fontweight=\"bold\")\n",
        "ax2.set_ylabel(\"Average Output Tokens\", fontsize=12, fontweight=\"bold\")\n",
        "ax2.set_title(\"Average Output Tokens per Query\", fontsize=13, fontweight=\"bold\")\n",
        "ax2.tick_params(axis=\"x\", rotation=45)\n",
        "ax2.grid(True, alpha=0.3, axis=\"y\")\n",
        "\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax2.text(\n",
        "        bar.get_x() + bar.get_width() / 2.,\n",
        "        height,\n",
        "        f\"{height:.0f}\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=9,\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 4: Radar/Spider Chart for Overall Performance\n",
        "\n",
        "from math import pi\n",
        "\n",
        "def create_radar_chart(df: pd.DataFrame):\n",
        "    \"\"\"Create a radar chart comparing normalized metrics.\"\"\"\n",
        "    \n",
        "    # Normalize metrics to 0-100 scale for comparison\n",
        "    # For latency, invert (lower is better) and normalize\n",
        "    metrics = [\"Syntactic Validity (%)\", \"Execution Success (%)\", \"Semantic Accuracy (%)\"]\n",
        "    \n",
        "    # Set up the radar chart\n",
        "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection=\"polar\"))\n",
        "    \n",
        "    # Compute angles for each metric\n",
        "    angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]\n",
        "    angles += angles[:1]  # Complete the circle\n",
        "    \n",
        "    # Plot each model\n",
        "    colors = plt.cm.tab10(range(len(df)))\n",
        "    \n",
        "    for idx, (_, row) in enumerate(df.iterrows()):\n",
        "        values = [row[metric] for metric in metrics]\n",
        "        values += values[:1]  # Complete the circle\n",
        "        \n",
        "        ax.plot(angles, values, \"o-\", linewidth=2, label=row[\"Model\"], color=colors[idx])\n",
        "        ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
        "    \n",
        "    # Customize\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(metrics, fontsize=11)\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.set_yticks([25, 50, 75, 100])\n",
        "    ax.set_yticklabels([\"25%\", \"50%\", \"75%\", \"100%\"], fontsize=9)\n",
        "    ax.grid(True)\n",
        "    \n",
        "    plt.title(\"Accuracy Metrics Radar Chart\", size=14, fontweight=\"bold\", pad=20)\n",
        "    plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "create_radar_chart(comparison_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 5: Trade-off Analysis (Accuracy vs Latency)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "# Scatter plot\n",
        "for _, row in comparison_df.iterrows():\n",
        "    ax.scatter(\n",
        "        row[\"Avg Latency (ms)\"],\n",
        "        row[\"Semantic Accuracy (%)\"],\n",
        "        s=300,\n",
        "        alpha=0.6,\n",
        "        label=row[\"Model\"],\n",
        "    )\n",
        "    ax.annotate(\n",
        "        row[\"Model\"],\n",
        "        (row[\"Avg Latency (ms)\"], row[\"Semantic Accuracy (%)\"]),\n",
        "        xytext=(5, 5),\n",
        "        textcoords=\"offset points\",\n",
        "        fontsize=10,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "ax.set_xlabel(\"Average Latency (ms)\", fontsize=12, fontweight=\"bold\")\n",
        "ax.set_ylabel(\"Semantic Accuracy (%)\", fontsize=12, fontweight=\"bold\")\n",
        "ax.set_title(\"Accuracy vs Latency Trade-off\", fontsize=14, fontweight=\"bold\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary Statistics\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATION RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for _, row in comparison_df.iterrows():\n",
        "    print(f\"\\n{row['Model']}:\")\n",
        "    print(f\"  Syntactic Validity: {row['Syntactic Validity (%)']:.2f}%\")\n",
        "    print(f\"  Execution Success: {row['Execution Success (%)']:.2f}%\")\n",
        "    print(f\"  Semantic Accuracy: {row['Semantic Accuracy (%)']:.2f}%\")\n",
        "    print(f\"  Avg Latency: {row['Avg Latency (ms)']:.2f} ms\")\n",
        "    print(f\"  Avg Input Tokens: {row['Avg Input Tokens']:.0f}\")\n",
        "    print(f\"  Avg Output Tokens: {row['Avg Output Tokens']:.0f}\")\n",
        "    print(f\"  Test Set Size: {row['Test Set Size']:.0f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Best performers\n",
        "print(\"\\nBEST PERFORMERS:\")\n",
        "print(f\"  Highest Semantic Accuracy: {comparison_df.loc[comparison_df['Semantic Accuracy (%)'].idxmax(), 'Model']} ({comparison_df['Semantic Accuracy (%)'].max():.2f}%)\")\n",
        "print(f\"  Fastest: {comparison_df.loc[comparison_df['Avg Latency (ms)'].idxmin(), 'Model']} ({comparison_df['Avg Latency (ms)'].min():.2f} ms)\")\n",
        "print(f\"  Most Syntactically Valid: {comparison_df.loc[comparison_df['Syntactic Validity (%)'].idxmax(), 'Model']} ({comparison_df['Syntactic Validity (%)'].max():.2f}%)\")\n",
        "print(f\"  Best Execution Success: {comparison_df.loc[comparison_df['Execution Success (%)'].idxmax(), 'Model']} ({comparison_df['Execution Success (%)'].max():.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Per-Question Analysis (Optional)\n",
        "\n",
        "To analyze per-question results, load the summary CSV files:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load detailed per-question results (optional)\n",
        "def load_summary_csvs(results_dir: Path) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Load all summary CSV files.\"\"\"\n",
        "    csv_files = list(results_dir.glob(\"*_summary.csv\"))\n",
        "    summaries = {}\n",
        "    \n",
        "    for csv_file in sorted(csv_files):\n",
        "        model_name = csv_file.stem.replace(\"_summary\", \"\")\n",
        "        df = pd.read_csv(csv_file)\n",
        "        \n",
        "        # Convert checkmarks to boolean\n",
        "        if \"syntactic_valid\" in df.columns:\n",
        "            df[\"syntactic_valid\"] = df[\"syntactic_valid\"].apply(lambda x: x == \"✓\")\n",
        "        if \"execution_success\" in df.columns:\n",
        "            df[\"execution_success\"] = df[\"execution_success\"].apply(lambda x: x == \"✓\")\n",
        "        if \"result_match\" in df.columns:\n",
        "            df[\"result_match\"] = df[\"result_match\"].apply(lambda x: x == \"✓\")\n",
        "        \n",
        "        summaries[model_name] = df\n",
        "        print(f\"Loaded {len(df)} records for {model_name}\")\n",
        "    \n",
        "    return summaries\n",
        "\n",
        "# Uncomment to load detailed results\n",
        "# detailed_results = load_summary_csvs(RESULTS_DIR)\n",
        "# print(f\"\\nLoaded {len(detailed_results)} detailed result sets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Compare per-question results across models (optional)\n",
        "# Uncomment to run\n",
        "\n",
        "# if 'detailed_results' in globals() and detailed_results:\n",
        "#     # Find questions where models differ\n",
        "#     all_ids = set()\n",
        "#     for df in detailed_results.values():\n",
        "#         all_ids.update(df['id'])\n",
        "#     \n",
        "#     comparison_detailed = []\n",
        "#     for qid in sorted(all_ids):\n",
        "#         row = {'id': qid}\n",
        "#         for model_name, df in detailed_results.items():\n",
        "#             model_row = df[df['id'] == qid]\n",
        "#             if not model_row.empty:\n",
        "#                 row[model_name] = '✓' if model_row.iloc[0]['result_match'] else '✗'\n",
        "#             else:\n",
        "#                 row[model_name] = 'N/A'\n",
        "#         comparison_detailed.append(row)\n",
        "#     \n",
        "#     comparison_detailed_df = pd.DataFrame(comparison_detailed)\n",
        "#     display(comparison_detailed_df.head(20))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
