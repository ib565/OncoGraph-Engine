{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation: Model-Agnostic Evaluation System\n",
    "\n",
    "This notebook evaluates baseline performance for multiple models:\n",
    "- **Gemini 2.0 Flash** (2-step pipeline)\n",
    "- **Gemini 2.5 Flash Lite** (2-step pipeline)\n",
    "- **Qwen3-4B-Instruct-2507** (base model, untuned)\n",
    "\n",
    "Metrics tracked:\n",
    "- Syntactic validity (passes RuleBasedValidator)\n",
    "- Execution success (runs on Neo4j)\n",
    "- Semantic accuracy (exact result match with gold Cypher)\n",
    "- Latency (ms)\n",
    "- Token usage (input/output tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & Configuration\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to path\n",
    "ROOT = Path(os.getcwd()).resolve()\n",
    "\n",
    "# Walk up until we find a directory with both pyproject.toml AND src/\n",
    "found_root = False\n",
    "while True:\n",
    "    has_pyproject = (ROOT / \"pyproject.toml\").exists()\n",
    "    has_src = (ROOT / \"src\").exists() and (ROOT / \"src\").is_dir()\n",
    "\n",
    "    if has_pyproject and has_src:\n",
    "        found_root = True\n",
    "        break\n",
    "\n",
    "    parent = ROOT.parent\n",
    "    if parent == ROOT:\n",
    "        break\n",
    "    ROOT = parent\n",
    "\n",
    "if not found_root:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find project root (directory with both pyproject.toml and src/). \"\n",
    "        f\"Current directory: {os.getcwd()}. \"\n",
    "        f\"Please set ROOT manually above.\"\n",
    "    )\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"src module verified at: {ROOT / 'src'}\")\n",
    "\n",
    "from src.pipeline.executor import Neo4jExecutor\n",
    "from src.pipeline.types import PipelineConfig\n",
    "from src.pipeline.validator import RuleBasedValidator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "# Models to evaluate (supports single model in list or multiple models)\n",
    "MODELS_TO_RUN = [\n",
    "    # \"gemini-2.0-flash\",\n",
    "    # \"gemini-2.5-flash-lite\",\n",
    "    \"qwen3-4b-it-2507-base\",\n",
    "]\n",
    "TEST_SUBSET_SIZE = 80  # Number of test records to evaluate (fits rate limits)\n",
    "RATE_LIMIT_RPM = 15  # Gemini rate limit: requests per minute (common for all Gemini models)\n",
    "\n",
    "# Path configuration\n",
    "BASE_DIR = ROOT\n",
    "DATA_DIR = BASE_DIR / \"finetuning\" / \"data\" / \"processed\" / \"splits\"\n",
    "EVAL_DIR = BASE_DIR / \"finetuning\" / \"evaluation\" / \"results\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Environment variables\n",
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.environ.get(\"NEO4J_USER\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.environ.get(\"NEO4J_PASSWORD\", \"\")\n",
    "GEMINI_API_KEY = os.environ.get(\"GOOGLE_API_KEY\", \"\")\n",
    "\n",
    "if not NEO4J_PASSWORD:\n",
    "    raise ValueError(\"NEO4J_PASSWORD environment variable not set\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set\")\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "print(f\"Test subset size: {TEST_SUBSET_SIZE}\")\n",
    "print(f\"Models to evaluate: {MODELS_TO_RUN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def load_test_set(n: int) -> list[dict[str, Any]]:\n",
    "    \"\"\"Load test set and optionally sample n records with stratified sampling.\"\"\"\n",
    "    test_file = DATA_DIR / \"test_sample.jsonl\"\n",
    "    if not test_file.exists():\n",
    "        raise FileNotFoundError(f\"Test file not found: {test_file}\")\n",
    "\n",
    "    records = []\n",
    "    with test_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            records.append(json.loads(line))\n",
    "\n",
    "    if n >= len(records):\n",
    "        return records\n",
    "\n",
    "    # Stratified sampling by template_id\n",
    "    records_by_template = defaultdict(list)\n",
    "    for record in records:\n",
    "        records_by_template[record[\"template_id\"]].append(record)\n",
    "\n",
    "    sampled = []\n",
    "    for _template_id, template_records in records_by_template.items():\n",
    "        num_to_sample = max(1, int(len(template_records) * (n / len(records))))\n",
    "        if num_to_sample > len(template_records):\n",
    "            num_to_sample = len(template_records)\n",
    "        sampled.extend(template_records[:num_to_sample])\n",
    "\n",
    "    # If we still need more, randomly sample remaining\n",
    "    if len(sampled) < n:\n",
    "        remaining = [r for r in records if r not in sampled]\n",
    "        import random\n",
    "\n",
    "        random.seed(42)\n",
    "        additional = random.sample(remaining, n - len(sampled))\n",
    "        sampled.extend(additional)\n",
    "\n",
    "    return sampled[:n]\n",
    "\n",
    "\n",
    "def extract_partial_matches(results: list[dict[str, Any]], model_name: str) -> list[dict[str, Any]]:\n",
    "    \"\"\"Extract records where syntactic and execution pass but result doesn't match.\"\"\"\n",
    "    partial = []\n",
    "    for r in results:\n",
    "        if r[\"syntactic_valid\"] and r[\"execution_success\"] and not r[\"result_match\"]:\n",
    "            partial.append(\n",
    "                {\n",
    "                    \"id\": r[\"id\"],\n",
    "                    \"question\": r[\"question\"],\n",
    "                    \"gold_cypher\": r[\"gold_cypher\"],\n",
    "                    \"generated_cypher\": r[\"generated_cypher\"],\n",
    "                    \"gold_rows\": r.get(\"gold_rows\", []),\n",
    "                    \"generated_rows\": r.get(\"generated_rows\", []),\n",
    "                    \"error\": r.get(\"error\"),\n",
    "                }\n",
    "            )\n",
    "    return partial\n",
    "\n",
    "\n",
    "def create_inspection_summary(results: list[dict[str, Any]], model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Create a readable summary DataFrame from results.\"\"\"\n",
    "    summary_data = []\n",
    "    for r in results:\n",
    "        error_val = r.get(\"error\") or \"\"\n",
    "        question_val = r.get(\"question\") or \"\"\n",
    "\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"id\": r.get(\"id\", \"\"),\n",
    "                \"question\": question_val,\n",
    "                \"syntactic_valid\": \"âœ“\" if r.get(\"syntactic_valid\") else \"âœ—\",\n",
    "                \"execution_success\": \"âœ“\" if r.get(\"execution_success\") else \"âœ—\",\n",
    "                \"result_match\": \"âœ“\" if r.get(\"result_match\") else \"âœ—\",\n",
    "                \"gold_rows\": len(r.get(\"gold_rows\", [])),\n",
    "                \"gen_rows\": len(r.get(\"generated_rows\", [])),\n",
    "                \"error\": error_val,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "def inspect_record(record: dict[str, Any], max_rows_display: int = 20):\n",
    "    \"\"\"Pretty-print a record for detailed inspection.\"\"\"\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"ID: {record.get('id', 'N/A')}\")\n",
    "    print(f\"Question: {record.get('question', 'N/A')}\")\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "\n",
    "    if record.get(\"error\"):\n",
    "        print(f\"ERROR: {record.get('error')}\")\n",
    "\n",
    "    gold_cypher = record.get(\"gold_cypher\", \"\")\n",
    "    gen_cypher = record.get(\"generated_cypher\", \"\")\n",
    "    gold_rows = record.get(\"gold_rows\", [])\n",
    "    gen_rows = record.get(\"generated_rows\", [])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"ðŸ“‹ GOLD CYPHER (Expected Reference)\")\n",
    "    print(\"=\" * 100)\n",
    "    if gold_cypher:\n",
    "        print(\"```cypher\")\n",
    "        for line in gold_cypher.split(\"\\n\"):\n",
    "            print(line)\n",
    "        print(\"```\")\n",
    "    else:\n",
    "        print(\"  (empty)\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š GOLD RESULTS: {len(gold_rows)} row(s)\")\n",
    "    print(\"-\" * 100)\n",
    "    if gold_rows:\n",
    "        for i, row in enumerate(gold_rows[:max_rows_display], 1):\n",
    "            row_str = \", \".join([f\"{k}={v!r}\" for k, v in row.items()])\n",
    "            print(f\"  [{i:3d}] {row_str}\")\n",
    "        if len(gold_rows) > max_rows_display:\n",
    "            print(f\"  ... ({len(gold_rows) - max_rows_display} more rows)\")\n",
    "    else:\n",
    "        print(\"  (no results)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"ðŸ”§ GENERATED CYPHER (Model Output)\")\n",
    "    print(\"=\" * 100)\n",
    "    if gen_cypher:\n",
    "        print(\"```cypher\")\n",
    "        for line in gen_cypher.split(\"\\n\"):\n",
    "            print(line)\n",
    "        print(\"```\")\n",
    "    else:\n",
    "        print(\"  (empty)\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š GENERATED RESULTS: {len(gen_rows)} row(s)\")\n",
    "    print(\"-\" * 100)\n",
    "    if gen_rows:\n",
    "        for i, row in enumerate(gen_rows[:max_rows_display], 1):\n",
    "            row_str = \", \".join([f\"{k}={v!r}\" for k, v in row.items()])\n",
    "            print(f\"  [{i:3d}] {row_str}\")\n",
    "        if len(gen_rows) > max_rows_display:\n",
    "            print(f\"  ... ({len(gen_rows) - max_rows_display} more rows)\")\n",
    "    else:\n",
    "        print(\"  (no results)\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Evaluation Harness and Model Adapters\n",
    "\n",
    "from finetuning.evaluation import (\n",
    "    Evaluator,\n",
    "    GeminiModelAdapter,\n",
    "    ModelAdapter,\n",
    "    QwenModelAdapter,\n",
    "    run_evaluation,\n",
    ")\n",
    "\n",
    "print(\"Evaluation harness and model adapters imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Set\n",
    "\n",
    "test_records = load_test_set(TEST_SUBSET_SIZE)\n",
    "print(f\"Loaded {len(test_records)} test records\")\n",
    "print(\"Template distribution:\")\n",
    "template_counts = Counter(r[\"template_id\"] for r in test_records)\n",
    "for template_id, count in sorted(template_counts.items()):\n",
    "    print(f\"  {template_id}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Shared Components\n",
    "\n",
    "config = PipelineConfig()\n",
    "validator = RuleBasedValidator(config=config)\n",
    "executor = Neo4jExecutor(\n",
    "    uri=NEO4J_URI,\n",
    "    user=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"Shared components initialized (validator, executor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluations for All Models\n",
    "\n",
    "\n",
    "def create_model_adapter(model_id: str) -> ModelAdapter:\n",
    "    \"\"\"Factory function to create the appropriate model adapter.\"\"\"\n",
    "    if model_id.startswith(\"gemini\"):\n",
    "        return GeminiModelAdapter(\n",
    "            model=model_id,\n",
    "            api_key=GEMINI_API_KEY,\n",
    "            temperature=0.1,\n",
    "            rate_limit_rpm=RATE_LIMIT_RPM,\n",
    "        )\n",
    "    elif model_id.startswith(\"qwen\"):\n",
    "        return QwenModelAdapter()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_id}\")\n",
    "\n",
    "\n",
    "# Store all results\n",
    "all_results: dict[str, list[dict[str, Any]]] = {}\n",
    "all_metrics: dict[str, dict[str, Any]] = {}\n",
    "\n",
    "# Run evaluation for each model\n",
    "for model_id in MODELS_TO_RUN:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {model_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Create model adapter\n",
    "    adapter = create_model_adapter(model_id)\n",
    "\n",
    "    # Create evaluator for this adapter\n",
    "    evaluator = Evaluator(validator, executor, adapter)\n",
    "\n",
    "    # Create checkpoint file path\n",
    "    checkpoint_file = EVAL_DIR / f\"{model_id}_checkpoint.jsonl\"\n",
    "\n",
    "    # Run evaluation\n",
    "    results = run_evaluation(\n",
    "        model_adapter=adapter,\n",
    "        test_records=test_records[:2],\n",
    "        checkpoint_file=checkpoint_file,\n",
    "        evaluator=evaluator,\n",
    "        checkpoint_interval=5,\n",
    "    )\n",
    "\n",
    "    # Store results and compute metrics\n",
    "    all_results[model_id] = results\n",
    "    all_metrics[model_id] = evaluator.aggregate_metrics(results)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n{model_id} Metrics:\")\n",
    "    for key, value in all_metrics[model_id].items():\n",
    "        print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All evaluations complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Results from Checkpoints (if notebook was restarted)\n",
    "\n",
    "# If all_results is not defined or empty, load from checkpoints\n",
    "if \"all_results\" not in globals() or not all_results:\n",
    "    print(\"Loading results from checkpoint files...\")\n",
    "    all_results = {}\n",
    "    all_metrics = {}\n",
    "\n",
    "    for model_id in MODELS_TO_RUN:\n",
    "        checkpoint_file = EVAL_DIR / f\"{model_id}_checkpoint.jsonl\"\n",
    "        if checkpoint_file.exists():\n",
    "            with checkpoint_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                results = [json.loads(line) for line in f]\n",
    "                all_results[model_id] = results\n",
    "\n",
    "                # Create a dummy adapter to get evaluator for metrics\n",
    "                adapter = create_model_adapter(model_id)\n",
    "                evaluator = Evaluator(validator, executor, adapter)\n",
    "                all_metrics[model_id] = evaluator.aggregate_metrics(results)\n",
    "                print(f\"  Loaded {len(results)} results for {model_id}\")\n",
    "        else:\n",
    "            print(f\"  No checkpoint found for {model_id}\")\n",
    "\n",
    "print(f\"\\nAvailable model results: {list(all_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Analysis & Comparison\n",
    "\n",
    "if not all_results:\n",
    "    raise ValueError(\"No results available. Run Cell 6 to evaluate models first.\")\n",
    "\n",
    "# Create comparison table for all evaluated models\n",
    "comparison_data = {\n",
    "    \"Metric\": [\n",
    "        \"Syntactic Validity (%)\",\n",
    "        \"Execution Success (%)\",\n",
    "        \"Semantic Accuracy (%)\",\n",
    "        \"Avg Latency (ms)\",\n",
    "        \"Avg Input Tokens\",\n",
    "        \"Avg Output Tokens\",\n",
    "        \"Total Input Tokens\",\n",
    "        \"Total Output Tokens\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Add a column for each model\n",
    "for model_id, metrics in all_metrics.items():\n",
    "    comparison_data[model_id] = [\n",
    "        metrics.get(\"syntactic_validity_pct\", 0),\n",
    "        metrics.get(\"execution_success_pct\", 0),\n",
    "        metrics.get(\"semantic_accuracy_pct\", 0),\n",
    "        metrics.get(\"avg_latency_ms\", 0),\n",
    "        metrics.get(\"avg_input_tokens\", 0),\n",
    "        metrics.get(\"avg_output_tokens\", 0),\n",
    "        metrics.get(\"total_input_tokens\", 0),\n",
    "        metrics.get(\"total_output_tokens\", 0),\n",
    "    ]\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE EVALUATION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Save summaries per model\n",
    "for model_id, results in all_results.items():\n",
    "    summary = create_inspection_summary(results, model_id)\n",
    "    summary_file = EVAL_DIR / f\"{model_id}_summary.csv\"\n",
    "    summary.to_csv(summary_file, index=False)\n",
    "    print(f\"  {model_id} summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Partial Matches for All Models\n",
    "\n",
    "all_partial_matches = {}\n",
    "\n",
    "for model_id, results in all_results.items():\n",
    "    partial = extract_partial_matches(results, model_id)\n",
    "    all_partial_matches[model_id] = partial\n",
    "\n",
    "    # Save partial matches for inspection\n",
    "    partial_file = EVAL_DIR / f\"partial_matches_{model_id}.jsonl\"\n",
    "    with partial_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for record in partial:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"  {model_id}: {len(partial)} partial matches\")\n",
    "\n",
    "print(\"\\nPartial matches saved:\")\n",
    "for model_id, partial in all_partial_matches.items():\n",
    "    print(f\"  {EVAL_DIR / f'partial_matches_{model_id}.jsonl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Detailed Inspection for All Models\n",
    "\n",
    "for model_id, partial_matches in all_partial_matches.items():\n",
    "    if partial_matches:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"DETAILED INSPECTION: {model_id.upper()} PARTIAL MATCHES (syntax + exec pass, result mismatch)\")\n",
    "        print(\"=\" * 100)\n",
    "        for i, record in enumerate(partial_matches[:10]):\n",
    "            print(f\"\\n--- Mismatch #{i} ({model_id}) ---\")\n",
    "            inspect_record(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Results Summary\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"test_set_size\": len(test_records),\n",
    "    \"models\": {},\n",
    "}\n",
    "\n",
    "# Add all evaluated models\n",
    "for model_id, metrics in all_metrics.items():\n",
    "    results_summary[\"models\"][model_id] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"result_count\": len(all_results.get(model_id, [])),\n",
    "    }\n",
    "\n",
    "results_file = EVAL_DIR / \"evaluation_results.json\"\n",
    "with results_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nFinal results summary saved to: {results_file}\")\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Inspection Tool\n",
    "\n",
    "\n",
    "def inspect_by_id(record_id: str, model_id: str | None = None):\n",
    "    \"\"\"Inspect a specific record by ID across all evaluated models.\n",
    "\n",
    "    Args:\n",
    "        record_id: The record ID to search for.\n",
    "        model_id: Optional model ID to search in. If None, searches all models.\n",
    "    \"\"\"\n",
    "    if model_id:\n",
    "        # Search in specific model\n",
    "        if model_id in all_results:\n",
    "            for record in all_results[model_id]:\n",
    "                if record.get(\"id\") == record_id:\n",
    "                    print(f\"\\nFound in {model_id}:\")\n",
    "                    inspect_record(record, max_rows_display=50)\n",
    "                    return\n",
    "            print(f\"Record with ID '{record_id}' not found in {model_id}.\")\n",
    "        else:\n",
    "            print(f\"Model '{model_id}' not found. Available models: {list(all_results.keys())}\")\n",
    "    else:\n",
    "        # Search across all models\n",
    "        found = False\n",
    "        for mid, results in all_results.items():\n",
    "            for record in results:\n",
    "                if record.get(\"id\") == record_id:\n",
    "                    print(f\"\\nFound in {mid}:\")\n",
    "                    inspect_record(record, max_rows_display=50)\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "        if not found:\n",
    "            print(f\"Record with ID '{record_id}' not found in any model.\")\n",
    "\n",
    "\n",
    "print(\"\\nUse inspect_by_id('RECORD_ID') to inspect a specific record in detail.\")\n",
    "print(\"Use inspect_by_id('RECORD_ID', 'model_id') to search in a specific model.\")\n",
    "print(\"Example: inspect_by_id('F5.2-000167-S1-P2')\")\n",
    "print(\"Example: inspect_by_id('F5.2-000167-S1-P2', 'gemini-2.0-flash')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
