{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation: Model-Agnostic Evaluation System\n",
    "\n",
    "This notebook evaluates baseline performance for multiple models:\n",
    "- **Gemini 2.0 Flash** (2-step pipeline)\n",
    "- **Gemini 2.5 Flash Lite** (2-step pipeline)\n",
    "- **Qwen3-4B-Instruct-2507** (base model, untuned)\n",
    "\n",
    "Metrics tracked:\n",
    "- Syntactic validity (passes RuleBasedValidator)\n",
    "- Execution success (runs on Neo4j)\n",
    "- Semantic accuracy (exact result match with gold Cypher)\n",
    "- Latency (ms)\n",
    "- Token usage (input/output tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\n",
      "Current working directory: c:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\finetuning\\notebooks\n",
      "src module verified at: C:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\src\n",
      "Base directory: C:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\n",
      "Data directory: C:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\finetuning\\data\\processed\\splits\n",
      "Evaluation directory: C:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\finetuning\\evaluation\\results\n",
      "Test subset size: 80\n",
      "Models to evaluate: ['qwen3-4b-it-2507-base']\n"
     ]
    }
   ],
   "source": [
    "# Setup & Configuration\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to path\n",
    "ROOT = Path(os.getcwd()).resolve()\n",
    "\n",
    "# Walk up until we find a directory with both pyproject.toml AND src/\n",
    "found_root = False\n",
    "while True:\n",
    "    has_pyproject = (ROOT / \"pyproject.toml\").exists()\n",
    "    has_src = (ROOT / \"src\").exists() and (ROOT / \"src\").is_dir()\n",
    "\n",
    "    if has_pyproject and has_src:\n",
    "        found_root = True\n",
    "        break\n",
    "\n",
    "    parent = ROOT.parent\n",
    "    if parent == ROOT:\n",
    "        break\n",
    "    ROOT = parent\n",
    "\n",
    "if not found_root:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find project root (directory with both pyproject.toml and src/). \"\n",
    "        f\"Current directory: {os.getcwd()}. \"\n",
    "        f\"Please set ROOT manually above.\"\n",
    "    )\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"src module verified at: {ROOT / 'src'}\")\n",
    "\n",
    "from src.pipeline.executor import Neo4jExecutor\n",
    "from src.pipeline.types import PipelineConfig\n",
    "from src.pipeline.validator import RuleBasedValidator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "# Models to evaluate (supports single model in list or multiple models)\n",
    "MODELS_TO_RUN = [\n",
    "    # \"gemini-2.0-flash\",\n",
    "    # \"gemini-2.5-flash-lite\",\n",
    "    \"qwen3-4b-it-2507-base\",\n",
    "]\n",
    "TEST_SUBSET_SIZE = 80  # Number of test records to evaluate (fits rate limits)\n",
    "RATE_LIMIT_RPM = 15  # Gemini rate limit: requests per minute (common for all Gemini models)\n",
    "\n",
    "# Path configuration\n",
    "BASE_DIR = ROOT\n",
    "DATA_DIR = BASE_DIR / \"finetuning\" / \"data\" / \"processed\" / \"splits\"\n",
    "EVAL_DIR = BASE_DIR / \"finetuning\" / \"evaluation\" / \"results\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Environment variables\n",
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.environ.get(\"NEO4J_USER\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.environ.get(\"NEO4J_PASSWORD\", \"\")\n",
    "GEMINI_API_KEY = os.environ.get(\"GOOGLE_API_KEY\", \"\")\n",
    "\n",
    "if not NEO4J_PASSWORD:\n",
    "    raise ValueError(\"NEO4J_PASSWORD environment variable not set\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable not set\")\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "print(f\"Test subset size: {TEST_SUBSET_SIZE}\")\n",
    "print(f\"Models to evaluate: {MODELS_TO_RUN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def load_test_set(n: int) -> list[dict[str, Any]]:\n",
    "    \"\"\"Load test set and optionally sample n records with stratified sampling.\"\"\"\n",
    "    test_file = DATA_DIR / \"test_sample.jsonl\"\n",
    "    if not test_file.exists():\n",
    "        raise FileNotFoundError(f\"Test file not found: {test_file}\")\n",
    "\n",
    "    records = []\n",
    "    with test_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            records.append(json.loads(line))\n",
    "\n",
    "    if n >= len(records):\n",
    "        return records\n",
    "\n",
    "    # Stratified sampling by template_id\n",
    "    records_by_template = defaultdict(list)\n",
    "    for record in records:\n",
    "        records_by_template[record[\"template_id\"]].append(record)\n",
    "\n",
    "    sampled = []\n",
    "    for _template_id, template_records in records_by_template.items():\n",
    "        num_to_sample = max(1, int(len(template_records) * (n / len(records))))\n",
    "        if num_to_sample > len(template_records):\n",
    "            num_to_sample = len(template_records)\n",
    "        sampled.extend(template_records[:num_to_sample])\n",
    "\n",
    "    # If we still need more, randomly sample remaining\n",
    "    if len(sampled) < n:\n",
    "        remaining = [r for r in records if r not in sampled]\n",
    "        import random\n",
    "\n",
    "        random.seed(42)\n",
    "        additional = random.sample(remaining, n - len(sampled))\n",
    "        sampled.extend(additional)\n",
    "\n",
    "    return sampled[:n]\n",
    "\n",
    "\n",
    "def extract_partial_matches(results: list[dict[str, Any]], model_name: str) -> list[dict[str, Any]]:\n",
    "    \"\"\"Extract records where syntactic and execution pass but result doesn't match.\"\"\"\n",
    "    partial = []\n",
    "    for r in results:\n",
    "        if r[\"syntactic_valid\"] and r[\"execution_success\"] and not r[\"result_match\"]:\n",
    "            partial.append(\n",
    "                {\n",
    "                    \"id\": r[\"id\"],\n",
    "                    \"question\": r[\"question\"],\n",
    "                    \"gold_cypher\": r[\"gold_cypher\"],\n",
    "                    \"generated_cypher\": r[\"generated_cypher\"],\n",
    "                    \"gold_rows\": r.get(\"gold_rows\", []),\n",
    "                    \"generated_rows\": r.get(\"generated_rows\", []),\n",
    "                    \"error\": r.get(\"error\"),\n",
    "                }\n",
    "            )\n",
    "    return partial\n",
    "\n",
    "\n",
    "def create_inspection_summary(results: list[dict[str, Any]], model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Create a readable summary DataFrame from results.\"\"\"\n",
    "    summary_data = []\n",
    "    for r in results:\n",
    "        error_val = r.get(\"error\") or \"\"\n",
    "        question_val = r.get(\"question\") or \"\"\n",
    "\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"id\": r.get(\"id\", \"\"),\n",
    "                \"question\": question_val,\n",
    "                \"syntactic_valid\": \"âœ“\" if r.get(\"syntactic_valid\") else \"âœ—\",\n",
    "                \"execution_success\": \"âœ“\" if r.get(\"execution_success\") else \"âœ—\",\n",
    "                \"result_match\": \"âœ“\" if r.get(\"result_match\") else \"âœ—\",\n",
    "                \"gold_rows\": len(r.get(\"gold_rows\", [])),\n",
    "                \"gen_rows\": len(r.get(\"generated_rows\", [])),\n",
    "                \"error\": error_val,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "def inspect_record(record: dict[str, Any], max_rows_display: int = 20):\n",
    "    \"\"\"Pretty-print a record for detailed inspection.\"\"\"\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"ID: {record.get('id', 'N/A')}\")\n",
    "    print(f\"Question: {record.get('question', 'N/A')}\")\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "\n",
    "    if record.get(\"error\"):\n",
    "        print(f\"ERROR: {record.get('error')}\")\n",
    "\n",
    "    gold_cypher = record.get(\"gold_cypher\", \"\")\n",
    "    gen_cypher = record.get(\"generated_cypher\", \"\")\n",
    "    gold_rows = record.get(\"gold_rows\", [])\n",
    "    gen_rows = record.get(\"generated_rows\", [])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"ðŸ“‹ GOLD CYPHER (Expected Reference)\")\n",
    "    print(\"=\" * 100)\n",
    "    if gold_cypher:\n",
    "        print(\"```cypher\")\n",
    "        for line in gold_cypher.split(\"\\n\"):\n",
    "            print(line)\n",
    "        print(\"```\")\n",
    "    else:\n",
    "        print(\"  (empty)\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š GOLD RESULTS: {len(gold_rows)} row(s)\")\n",
    "    print(\"-\" * 100)\n",
    "    if gold_rows:\n",
    "        for i, row in enumerate(gold_rows[:max_rows_display], 1):\n",
    "            row_str = \", \".join([f\"{k}={v!r}\" for k, v in row.items()])\n",
    "            print(f\"  [{i:3d}] {row_str}\")\n",
    "        if len(gold_rows) > max_rows_display:\n",
    "            print(f\"  ... ({len(gold_rows) - max_rows_display} more rows)\")\n",
    "    else:\n",
    "        print(\"  (no results)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"ðŸ”§ GENERATED CYPHER (Model Output)\")\n",
    "    print(\"=\" * 100)\n",
    "    if gen_cypher:\n",
    "        print(\"```cypher\")\n",
    "        for line in gen_cypher.split(\"\\n\"):\n",
    "            print(line)\n",
    "        print(\"```\")\n",
    "    else:\n",
    "        print(\"  (empty)\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š GENERATED RESULTS: {len(gen_rows)} row(s)\")\n",
    "    print(\"-\" * 100)\n",
    "    if gen_rows:\n",
    "        for i, row in enumerate(gen_rows[:max_rows_display], 1):\n",
    "            row_str = \", \".join([f\"{k}={v!r}\" for k, v in row.items()])\n",
    "            print(f\"  [{i:3d}] {row_str}\")\n",
    "        if len(gen_rows) > max_rows_display:\n",
    "            print(f\"  ... ({len(gen_rows) - max_rows_display} more rows)\")\n",
    "    else:\n",
    "        print(\"  (no results)\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation harness and model adapters imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import Evaluation Harness and Model Adapters\n",
    "\n",
    "from finetuning.evaluation import (\n",
    "    Evaluator,\n",
    "    GeminiModelAdapter,\n",
    "    ModelAdapter,\n",
    "    QwenModelAdapter,\n",
    "    run_evaluation,\n",
    ")\n",
    "\n",
    "print(\"Evaluation harness and model adapters imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 test records\n",
      "Template distribution:\n",
      "  f1_1_targets_gene: 2\n",
      "  f1_2_genes_for_therapy: 3\n",
      "  f1_3_gene_of_variant: 2\n",
      "  f1_4_variants_of_gene: 2\n",
      "  f2_1_targets_properties: 2\n",
      "  f2_2_gene_affects_therapy_disease: 5\n",
      "  f2_2_gene_affects_therapy_disease_resistance: 4\n",
      "  f2_2_gene_affects_therapy_disease_sensitivity: 5\n",
      "  f2_2_variant_affects_therapy_disease: 5\n",
      "  f2_2_variant_token_affects_therapy_disease: 5\n",
      "  f2_3_evidential_pmids_variant_resistance: 2\n",
      "  f2_3_evidential_pmids_variant_sensitivity: 2\n",
      "  f2_4_variant_node_property: 2\n",
      "  f3_1_therapies_union_genes: 1\n",
      "  f3_2_therapies_intersection_genes: 2\n",
      "  f3_3_therapies_target_a_not_b: 2\n",
      "  f4_1_target_validation_gene_disease: 4\n",
      "  f4_2_alternative_therapies_from_resistance_genes: 4\n",
      "  f5_1_biomarkers_in_disease: 3\n",
      "  f5_2_therapies_with_biomarker_evidence_in_disease: 4\n",
      "  f6_1_resistance_genes_for_union_therapies_in_disease: 4\n",
      "  f6_2_therapies_target_gene_not_other_with_disease_evidence: 6\n",
      "  f6_3_therapies_target_gene_with_variant_resistance_in_disease: 4\n",
      "  f6_4_alternative_therapies_for_resistance_in_disease: 5\n"
     ]
    }
   ],
   "source": [
    "# Load Test Set\n",
    "\n",
    "test_records = load_test_set(TEST_SUBSET_SIZE)\n",
    "print(f\"Loaded {len(test_records)} test records\")\n",
    "print(\"Template distribution:\")\n",
    "template_counts = Counter(r[\"template_id\"] for r in test_records)\n",
    "for template_id, count in sorted(template_counts.items()):\n",
    "    print(f\"  {template_id}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared components initialized (validator, executor)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Shared Components\n",
    "\n",
    "config = PipelineConfig()\n",
    "validator = RuleBasedValidator(config=config)\n",
    "executor = Neo4jExecutor(\n",
    "    uri=NEO4J_URI,\n",
    "    user=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"Shared components initialized (validator, executor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating: qwen3-4b-it-2507-base\n",
      "================================================================================\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find cuobjdump.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n",
      "c:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find nvdisasm.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n",
      "[torchao|WARNING]Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "W1102 21:20:14.596000 37028 venv\\Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading Qwen model: unsloth/Qwen3-4B-Instruct-2507...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:348: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE_TORCH}:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.12: Fast Qwen3 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3050 Laptop GPU. Num GPUs = 1. Max memory: 4.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557a0c6f882549a2849209efd12fbeaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Create model adapter\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m adapter = \u001b[43mcreate_model_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Create evaluator for this adapter\u001b[39;00m\n\u001b[32m     33\u001b[39m evaluator = Evaluator(validator, executor, adapter)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcreate_model_adapter\u001b[39m\u001b[34m(model_id)\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GeminiModelAdapter(\n\u001b[32m      8\u001b[39m         model=model_id,\n\u001b[32m      9\u001b[39m         api_key=GEMINI_API_KEY,\n\u001b[32m     10\u001b[39m         temperature=\u001b[32m0.1\u001b[39m,\n\u001b[32m     11\u001b[39m         rate_limit_rpm=RATE_LIMIT_RPM,\n\u001b[32m     12\u001b[39m     )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_id.startswith(\u001b[33m\"\u001b[39m\u001b[33mqwen\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQwenModelAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\OncoGraph\\OncoGraph Agent\\finetuning\\evaluation\\model_adapters.py:241\u001b[39m, in \u001b[36mQwenModelAdapter.__init__\u001b[39m\u001b[34m(self, model_name, model_id, max_seq_length, temperature, top_p, top_k, max_new_tokens)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28mself\u001b[39m.max_new_tokens = max_new_tokens\n\u001b[32m    240\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading Qwen model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m FastLanguageModel.for_inference(\u001b[38;5;28mself\u001b[39m.model)  \u001b[38;5;66;03m# Enable inference optimizations\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Ensure the tokenizer uses the correct Qwen3 instruct chat template\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\unsloth\\models\\loader.py:482\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fast_inference:\n\u001b[32m    480\u001b[39m     fast_inference, model_name = fast_inference_setup(model_name, model_config)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m model, tokenizer = \u001b[43mdispatch_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    506\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\unsloth\\models\\qwen3.py:436\u001b[39m, in \u001b[36mFastQwen3Model.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained\u001b[39m(  \u001b[38;5;66;03m#TODO: Change after release\u001b[39;00m\n\u001b[32m    423\u001b[39m     model_name        = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen3-7B\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m     **kwargs,\n\u001b[32m    435\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastLlamaModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mFastQwen3Model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\unsloth\\models\\llama.py:2034\u001b[39m, in \u001b[36mFastLlamaModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, qat_scheme, **kwargs)\u001b[39m\n\u001b[32m   2021\u001b[39m     model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[32m   2022\u001b[39m         model_name,\n\u001b[32m   2023\u001b[39m         device_map              = device_map,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2031\u001b[39m         **kwargs,\n\u001b[32m   2032\u001b[39m     )\n\u001b[32m   2033\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[32m-> \u001b[39m\u001b[32m2034\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# torch_dtype             = dtype, # transformers changed torch_dtype to dtype\u001b[39;49;00m\n\u001b[32m   2038\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[32m   2039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meager\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2043\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2045\u001b[39m     model.fast_generate = model.generate\n\u001b[32m   2046\u001b[39m     model.fast_generate_batches = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5029\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5027\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   5028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5029\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5031\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   5032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:1365\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1362\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1368\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ishui\\Desktop\\OncoGraph\\OncoGraph Agent\\venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:127\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "# Run Evaluations for All Models\n",
    "\n",
    "\n",
    "def create_model_adapter(model_id: str) -> ModelAdapter:\n",
    "    \"\"\"Factory function to create the appropriate model adapter.\"\"\"\n",
    "    if model_id.startswith(\"gemini\"):\n",
    "        return GeminiModelAdapter(\n",
    "            model=model_id,\n",
    "            api_key=GEMINI_API_KEY,\n",
    "            temperature=0.1,\n",
    "            rate_limit_rpm=RATE_LIMIT_RPM,\n",
    "        )\n",
    "    elif model_id.startswith(\"qwen\"):\n",
    "        return QwenModelAdapter()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_id}\")\n",
    "\n",
    "\n",
    "# Store all results\n",
    "all_results: dict[str, list[dict[str, Any]]] = {}\n",
    "all_metrics: dict[str, dict[str, Any]] = {}\n",
    "\n",
    "# Run evaluation for each model\n",
    "for model_id in MODELS_TO_RUN:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {model_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Create model adapter\n",
    "    adapter = create_model_adapter(model_id)\n",
    "\n",
    "    # Create evaluator for this adapter\n",
    "    evaluator = Evaluator(validator, executor, adapter)\n",
    "\n",
    "    # Create checkpoint file path\n",
    "    checkpoint_file = EVAL_DIR / f\"{model_id}_checkpoint.jsonl\"\n",
    "\n",
    "    # Run evaluation\n",
    "    results = run_evaluation(\n",
    "        model_adapter=adapter,\n",
    "        test_records=test_records[:2],\n",
    "        checkpoint_file=checkpoint_file,\n",
    "        evaluator=evaluator,\n",
    "        checkpoint_interval=50,\n",
    "    )\n",
    "\n",
    "    # Store results and compute metrics\n",
    "    all_results[model_id] = results\n",
    "    all_metrics[model_id] = evaluator.aggregate_metrics(results)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n{model_id} Metrics:\")\n",
    "    for key, value in all_metrics[model_id].items():\n",
    "        print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All evaluations complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Results from Checkpoints (if notebook was restarted)\n",
    "\n",
    "# If all_results is not defined or empty, load from checkpoints\n",
    "if \"all_results\" not in globals() or not all_results:\n",
    "    print(\"Loading results from checkpoint files...\")\n",
    "    all_results = {}\n",
    "    all_metrics = {}\n",
    "\n",
    "    for model_id in MODELS_TO_RUN:\n",
    "        checkpoint_file = EVAL_DIR / f\"{model_id}_checkpoint.jsonl\"\n",
    "        if checkpoint_file.exists():\n",
    "            with checkpoint_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                results = [json.loads(line) for line in f]\n",
    "                all_results[model_id] = results\n",
    "\n",
    "                # Create a dummy adapter to get evaluator for metrics\n",
    "                adapter = create_model_adapter(model_id)\n",
    "                evaluator = Evaluator(validator, executor, adapter)\n",
    "                all_metrics[model_id] = evaluator.aggregate_metrics(results)\n",
    "                print(f\"  Loaded {len(results)} results for {model_id}\")\n",
    "        else:\n",
    "            print(f\"  No checkpoint found for {model_id}\")\n",
    "\n",
    "print(f\"\\nAvailable model results: {list(all_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Analysis & Comparison\n",
    "\n",
    "if not all_results:\n",
    "    raise ValueError(\"No results available. Run Cell 6 to evaluate models first.\")\n",
    "\n",
    "# Create comparison table for all evaluated models\n",
    "comparison_data = {\n",
    "    \"Metric\": [\n",
    "        \"Syntactic Validity (%)\",\n",
    "        \"Execution Success (%)\",\n",
    "        \"Semantic Accuracy (%)\",\n",
    "        \"Avg Latency (ms)\",\n",
    "        \"Avg Input Tokens\",\n",
    "        \"Avg Output Tokens\",\n",
    "        \"Total Input Tokens\",\n",
    "        \"Total Output Tokens\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Add a column for each model\n",
    "for model_id, metrics in all_metrics.items():\n",
    "    comparison_data[model_id] = [\n",
    "        metrics.get(\"syntactic_validity_pct\", 0),\n",
    "        metrics.get(\"execution_success_pct\", 0),\n",
    "        metrics.get(\"semantic_accuracy_pct\", 0),\n",
    "        metrics.get(\"avg_latency_ms\", 0),\n",
    "        metrics.get(\"avg_input_tokens\", 0),\n",
    "        metrics.get(\"avg_output_tokens\", 0),\n",
    "        metrics.get(\"total_input_tokens\", 0),\n",
    "        metrics.get(\"total_output_tokens\", 0),\n",
    "    ]\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE EVALUATION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Save summaries per model\n",
    "for model_id, results in all_results.items():\n",
    "    summary = create_inspection_summary(results, model_id)\n",
    "    summary_file = EVAL_DIR / f\"{model_id}_summary.csv\"\n",
    "    summary.to_csv(summary_file, index=False)\n",
    "    print(f\"  {model_id} summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Partial Matches for All Models\n",
    "\n",
    "all_partial_matches = {}\n",
    "\n",
    "for model_id, results in all_results.items():\n",
    "    partial = extract_partial_matches(results, model_id)\n",
    "    all_partial_matches[model_id] = partial\n",
    "\n",
    "    # Save partial matches for inspection\n",
    "    partial_file = EVAL_DIR / f\"partial_matches_{model_id}.jsonl\"\n",
    "    with partial_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for record in partial:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"  {model_id}: {len(partial)} partial matches\")\n",
    "\n",
    "print(\"\\nPartial matches saved:\")\n",
    "for model_id, partial in all_partial_matches.items():\n",
    "    print(f\"  {EVAL_DIR / f'partial_matches_{model_id}.jsonl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Detailed Inspection for All Models\n",
    "\n",
    "for model_id, partial_matches in all_partial_matches.items():\n",
    "    if partial_matches:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"DETAILED INSPECTION: {model_id.upper()} PARTIAL MATCHES (syntax + exec pass, result mismatch)\")\n",
    "        print(\"=\" * 100)\n",
    "        for i, record in enumerate(partial_matches[:10]):\n",
    "            print(f\"\\n--- Mismatch #{i} ({model_id}) ---\")\n",
    "            inspect_record(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Results Summary\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"test_set_size\": len(test_records),\n",
    "    \"models\": {},\n",
    "}\n",
    "\n",
    "# Add all evaluated models\n",
    "for model_id, metrics in all_metrics.items():\n",
    "    results_summary[\"models\"][model_id] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"result_count\": len(all_results.get(model_id, [])),\n",
    "    }\n",
    "\n",
    "results_file = EVAL_DIR / \"evaluation_results.json\"\n",
    "with results_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nFinal results summary saved to: {results_file}\")\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Inspection Tool\n",
    "\n",
    "\n",
    "def inspect_by_id(record_id: str, model_id: str | None = None):\n",
    "    \"\"\"Inspect a specific record by ID across all evaluated models.\n",
    "\n",
    "    Args:\n",
    "        record_id: The record ID to search for.\n",
    "        model_id: Optional model ID to search in. If None, searches all models.\n",
    "    \"\"\"\n",
    "    if model_id:\n",
    "        # Search in specific model\n",
    "        if model_id in all_results:\n",
    "            for record in all_results[model_id]:\n",
    "                if record.get(\"id\") == record_id:\n",
    "                    print(f\"\\nFound in {model_id}:\")\n",
    "                    inspect_record(record, max_rows_display=50)\n",
    "                    return\n",
    "            print(f\"Record with ID '{record_id}' not found in {model_id}.\")\n",
    "        else:\n",
    "            print(f\"Model '{model_id}' not found. Available models: {list(all_results.keys())}\")\n",
    "    else:\n",
    "        # Search across all models\n",
    "        found = False\n",
    "        for mid, results in all_results.items():\n",
    "            for record in results:\n",
    "                if record.get(\"id\") == record_id:\n",
    "                    print(f\"\\nFound in {mid}:\")\n",
    "                    inspect_record(record, max_rows_display=50)\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "        if not found:\n",
    "            print(f\"Record with ID '{record_id}' not found in any model.\")\n",
    "\n",
    "\n",
    "print(\"\\nUse inspect_by_id('RECORD_ID') to inspect a specific record in detail.\")\n",
    "print(\"Use inspect_by_id('RECORD_ID', 'model_id') to search in a specific model.\")\n",
    "print(\"Example: inspect_by_id('F5.2-000167-S1-P2')\")\n",
    "print(\"Example: inspect_by_id('F5.2-000167-S1-P2', 'gemini-2.0-flash')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
