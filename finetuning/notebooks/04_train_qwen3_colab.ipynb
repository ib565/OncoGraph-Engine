{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9b72c5",
   "metadata": {},
   "source": [
    "# Fine-tune Qwen3-4B for OncoGraph Cypher Generation (Colab)\n",
    "\n",
    "This notebook fine-tunes Qwen3-4B-Instruct on the OncoGraph QA→Cypher dataset using Unsloth.\n",
    "\n",
    "**Training Strategy:**\n",
    "- Model: `unsloth/Qwen3-4B-Instruct-2507`\n",
    "- Method: LoRA (Low-Rank Adaptation) with 4-bit quantization\n",
    "- Loss: Only on assistant responses (Cypher queries)\n",
    "- Output: Raw Cypher queries only (no markdown, no explanations)\n",
    "\n",
    "**Data:** `train_sample.jsonl` with `question` → `cypher` pairs\n",
    "\n",
    "**Save Targets:**\n",
    "- Local: LoRA adapters + optional merged 16-bit/4-bit\n",
    "- Hugging Face: `ib565/oncograph` (public repo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe31a7",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install dependencies with pinned versions matching Unsloth's Qwen3 example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0461a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Enable GPU runtime (Runtime > Change runtime type > GPU)\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# Detect torch version for xformers compatibility\n",
    "import re\n",
    "v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "print(f\"Torch version: {torch.__version__}, xformers: {xformers}\")\n",
    "\n",
    "# Install dependencies (following Unsloth Qwen3 example)\n",
    "# Note: Using !pip (shell command) allows variable substitution in Colab\n",
    "!pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "!pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "\n",
    "# Set environment variables\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11fe88",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set training hyperparameters and model/dataset paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc2327",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"unsloth/Qwen3-4B-Instruct-2507\"\n",
    "HUB_REPO = \"ib565/oncograph\"  # Public HF repo\n",
    "\n",
    "# Dataset configuration\n",
    "SUBSET_SIZE = 2000  # Use first 2000 samples for initial run (set to None for full dataset)\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16  # Safe for T4 (15GB)\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0\n",
    "\n",
    "# Training configuration\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 2 * 4 = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 5\n",
    "MAX_STEPS = 200  # For smoke test run (set num_train_epochs=1 for full training)\n",
    "OPTIMIZER = \"adamw_8bit\"\n",
    "WEIGHT_DECAY = 0.001\n",
    "LR_SCHEDULER_TYPE = \"linear\"\n",
    "SEED = 3407\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Hub repo: {HUB_REPO}\")\n",
    "print(f\"  Subset size: {SUBSET_SIZE}\")\n",
    "print(f\"  LoRA r: {LORA_R}, alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}, GA: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}, Max steps: {MAX_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e528d9",
   "metadata": {},
   "source": [
    "## 3. Clone Repository and Setup\n",
    "\n",
    "Clone the repository to access the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335452f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Clone repository (same approach as evaluation notebook)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = \"https://github.com/ib565/OncoGraph-Engine.git\"\n",
    "REPO_DIR = \"/content/OncoGraph-Engine\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !rm -rf {REPO_DIR}\n",
    "\n",
    "!git clone -b fine-tuning {REPO_URL} {REPO_DIR}\n",
    "%cd {REPO_DIR}\n",
    "\n",
    "# Verify structure\n",
    "print(\"Project structure:\")\n",
    "print(f\"  pyproject.toml exists: {(Path(REPO_DIR) / 'pyproject.toml').exists()}\")\n",
    "print(f\"  src/ exists: {(Path(REPO_DIR) / 'src').exists()}\")\n",
    "print(f\"  finetuning/ exists: {(Path(REPO_DIR) / 'finetuning').exists()}\")\n",
    "\n",
    "# Set path to training data\n",
    "train_data_path = Path(REPO_DIR) / \"finetuning\" / \"data\" / \"processed\" / \"splits\" / \"train_sample.jsonl\"\n",
    "print(f\"\\nTraining data path: {train_data_path}\")\n",
    "print(f\"File exists: {train_data_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01f870",
   "metadata": {},
   "source": [
    "## 4. Load Training Data\n",
    "\n",
    "Load `train_sample.jsonl` from the cloned repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd738a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL data from cloned repo\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open(train_data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} samples from {train_data_path}\")\n",
    "if len(data) > 0:\n",
    "    print(f\"Sample keys: {list(data[0].keys())}\")\n",
    "    print(f\"First question: {data[0].get('question', 'N/A')[:100]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd339b",
   "metadata": {},
   "source": [
    "## 5. Define System Prompt\n",
    "\n",
    "Use the same system prompt as the evaluation pipeline to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfea6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# System prompt matching the evaluation pipeline (QwenModelAdapter)\n",
    "MINIMAL_SCHEMA = \"\"\"Graph schema:\n",
    "- Nodes: Gene(symbol), Variant(name), Therapy(name), Disease(name), Biomarker\n",
    "- Relationships: (Variant)-[:VARIANT_OF]->(Gene), (Therapy)-[:TARGETS]->(Gene),\n",
    "(Biomarker)-[:AFFECTS_RESPONSE_TO]->(Therapy)\n",
    "- Properties: effect, disease_name, pmids, moa, ref_sources, ref_ids, ref_urls\n",
    "- Return: Always include LIMIT, no parameters ($variables), use coalesce for arrays\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are an expert Cypher query translator for oncology data.\n",
    "\n",
    "{MINIMAL_SCHEMA}\n",
    "\n",
    "Rules:\n",
    "- Return only Cypher query (no markdown, no explanation)\n",
    "- Include RETURN clause and LIMIT\n",
    "- Use toLower() for case-insensitive matching\n",
    "- Wrap arrays with coalesce(..., []) before any()/all()\n",
    "- For disease filters, use token-based CONTAINS matching\n",
    "\"\"\"\n",
    "\n",
    "print(\"System prompt defined (matches evaluation pipeline):\")\n",
    "print(SYSTEM_PROMPT[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014b78c",
   "metadata": {},
   "source": [
    "## 6. Format Dataset\n",
    "\n",
    "Convert JSONL data to conversations format and apply Qwen3 chat template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0b7d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert to conversations format\n",
    "conversations = []\n",
    "for item in data:\n",
    "    if 'question' in item and 'cypher' in item:\n",
    "        conversations.append({\n",
    "            \"conversations\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": item[\"question\"]},\n",
    "                {\"role\": \"assistant\", \"content\": item[\"cypher\"]}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "# Optionally limit to subset for first run\n",
    "if SUBSET_SIZE is not None and len(conversations) > SUBSET_SIZE:\n",
    "    conversations = conversations[:SUBSET_SIZE]\n",
    "    print(f\"Limited to first {SUBSET_SIZE} samples\")\n",
    "\n",
    "print(f\"Total conversations: {len(conversations)}\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_list(conversations)\n",
    "print(f\"Dataset created: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53f325",
   "metadata": {},
   "source": [
    "## 7. Load Model and Tokenizer\n",
    "\n",
    "Load Qwen3-4B with Unsloth optimizations and apply chat template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f3021",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,  # Use LoRA\n",
    "    # token=\"hf_...\",  # Uncomment if using gated models\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998cc76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Apply Qwen3 chat template\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen3-instruct\",\n",
    ")\n",
    "\n",
    "print(\"Chat template applied: qwen3-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174bcb4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get PEFT model with LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
    "    random_state=SEED,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(f\"LoRA model configured: r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a04021",
   "metadata": {},
   "source": [
    "## 8. Prepare Dataset for Training\n",
    "\n",
    "Apply chat template to format conversations into text, then use `train_on_responses_only` to mask loss on user inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data format (converts to standard format if needed)\n",
    "from unsloth.chat_templates import standardize_data_formats\n",
    "\n",
    "dataset = standardize_data_formats(dataset)\n",
    "print(\"Data format standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708513e8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Apply chat template to format conversations into text\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) \n",
    "             for convo in convos]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "print(f\"Chat template applied to {len(dataset)} samples\")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\nSample formatted text (first 500 chars):\")\n",
    "print(dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995c978",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Use Unsloth's train_on_responses_only to mask loss on user inputs\n",
    "# This trains only on assistant responses (Cypher queries)\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "print(\"Preparing trainer to train only on assistant responses...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20316e7",
   "metadata": {},
   "source": [
    "## 9. Setup Trainer\n",
    "\n",
    "Configure SFTTrainer with training arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b2e43",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,  # Can set up evaluation if needed\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        # num_train_epochs=1,  # Uncomment for full training (set max_steps=None)\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        logging_steps=1,\n",
    "        optim=OPTIMIZER,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",  # Use \"wandb\" or \"tensorboard\" if desired\n",
    "        output_dir=CHECKPOINT_DIR,\n",
    "        save_strategy=\"steps\",  # Save every N steps\n",
    "        save_steps=50,  # Save checkpoint every 50 steps\n",
    "        save_total_limit=3,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Apply train_on_responses_only to mask user input loss\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|im_start|>user\\n\",\n",
    "    response_part=\"<|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "print(\"Trainer configured with assistant-only loss masking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0536b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Verify label masking - check a sample\n",
    "sample_idx = 100 if len(dataset) > 100 else 0\n",
    "sample = trainer.train_dataset[sample_idx]\n",
    "\n",
    "print(\"Sample input_ids (first 200 chars decoded):\")\n",
    "print(tokenizer.decode(sample[\"input_ids\"])[:200])\n",
    "\n",
    "print(\"\\nSample labels (-100 = masked, token id = training target):\")\n",
    "labels_str = tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in sample[\"labels\"]])\n",
    "print(labels_str[:300])\n",
    "\n",
    "# Count masked vs non-masked tokens\n",
    "masked = sum(1 for x in sample[\"labels\"] if x == -100)\n",
    "non_masked = sum(1 for x in sample[\"labels\"] if x != -100)\n",
    "print(f\"\\nMasked tokens: {masked}, Training tokens: {non_masked}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd392b",
   "metadata": {},
   "source": [
    "## 10. Train Model\n",
    "\n",
    "Start training. To resume from a checkpoint, use `trainer.train(resume_from_checkpoint=True)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f0cdc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Train the model (will automatically save checkpoints)\n",
    "print(\"Starting training...\")\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Optional: Resume from checkpoint if needed\n",
    "# trainer_stats = trainer.train(resume_from_checkpoint=True)  # Uncomment to resume\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training stats: {trainer_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72c0c8",
   "metadata": {},
   "source": [
    "## 11. Save Model Locally\n",
    "\n",
    "Save LoRA adapters and optionally merged models (16-bit or 4-bit) for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fcda9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Save LoRA adapters (only the trained weights, not full model)\n",
    "LORA_OUTPUT_DIR = \"lora_oncograph_qwen3_4b\"\n",
    "\n",
    "model.save_pretrained(LORA_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(LORA_OUTPUT_DIR)\n",
    "\n",
    "print(f\"LoRA adapters saved to: {LORA_OUTPUT_DIR}\")\n",
    "print(\"\\nNote: This saves only the LoRA weights. To load for inference:\")\n",
    "print(f\"  model, tokenizer = FastLanguageModel.from_pretrained('{LORA_OUTPUT_DIR}', load_in_4bit=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1f087",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Optional: Save merged 16-bit model (larger, but easier to deploy)\n",
    "SAVE_MERGED_16BIT = False  # Set to True to save merged 16-bit\n",
    "\n",
    "if SAVE_MERGED_16BIT:\n",
    "    MERGED_16BIT_DIR = \"oncograph_qwen3_4b_16bit\"\n",
    "    model.save_pretrained_merged(MERGED_16BIT_DIR, tokenizer, save_method=\"merged_16bit\")\n",
    "    print(f\"Merged 16-bit model saved to: {MERGED_16BIT_DIR}\")\n",
    "else:\n",
    "    print(\"Skipping merged 16-bit save (set SAVE_MERGED_16BIT=True to enable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10152bae",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Optional: Save merged 4-bit model (smallest, quantized)\n",
    "SAVE_MERGED_4BIT = False  # Set to True to save merged 4-bit\n",
    "\n",
    "if SAVE_MERGED_4BIT:\n",
    "    MERGED_4BIT_DIR = \"oncograph_qwen3_4b_4bit\"\n",
    "    model.save_pretrained_merged(MERGED_4BIT_DIR, tokenizer, save_method=\"merged_4bit\")\n",
    "    print(f\"Merged 4-bit model saved to: {MERGED_4BIT_DIR}\")\n",
    "else:\n",
    "    print(\"Skipping merged 4-bit save (set SAVE_MERGED_4BIT=True to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10b2e8",
   "metadata": {},
   "source": [
    "## 12. Push to Hugging Face Hub\n",
    "\n",
    "Upload model to `ib565/oncograph` (public repo). Handles repo creation and errors gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129cb97",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login, create_repo, HfApi\n",
    "import os\n",
    "\n",
    "# Try to get HF token from environment or prompt user\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    print(\"HF_TOKEN not found in environment. Please login:\")\n",
    "    login()  # This will prompt for token\n",
    "else:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Logged in with token from environment\")\n",
    "\n",
    "api = HfApi()\n",
    "print(f\"\\nPreparing to push to: {HUB_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d6bc0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create repo if it doesn't exist (gracefully handle if it does)\n",
    "try:\n",
    "    create_repo(HUB_REPO, private=False, exist_ok=True, token=HF_TOKEN if HF_TOKEN else None)\n",
    "    print(f\"Repository '{HUB_REPO}' is ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not create repo (might already exist): {e}\")\n",
    "\n",
    "# Push LoRA adapters\n",
    "try:\n",
    "    print(f\"\\nPushing LoRA adapters to {HUB_REPO}...\")\n",
    "    model.push_to_hub(HUB_REPO, token=HF_TOKEN if HF_TOKEN else None)\n",
    "    tokenizer.push_to_hub(HUB_REPO, token=HF_TOKEN if HF_TOKEN else None)\n",
    "    print(f\"✓ LoRA adapters pushed successfully to: https://huggingface.co/{HUB_REPO}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error pushing LoRA adapters: {e}\")\n",
    "    print(\"Model is still saved locally. You can try pushing manually later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8190686",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Optional: Push merged 16-bit model (if saved)\n",
    "PUSH_MERGED_16BIT = False  # Set to True to push merged 16-bit\n",
    "\n",
    "if SAVE_MERGED_16BIT and PUSH_MERGED_16BIT:\n",
    "    try:\n",
    "        print(f\"\\nPushing merged 16-bit model to {HUB_REPO}-16bit...\")\n",
    "        model.push_to_hub_merged(f\"{HUB_REPO}-16bit\", tokenizer, save_method=\"merged_16bit\", \n",
    "                                  token=HF_TOKEN if HF_TOKEN else None)\n",
    "        print(f\"✓ Merged 16-bit model pushed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error pushing merged 16-bit: {e}\")\n",
    "else:\n",
    "    print(\"Skipping merged 16-bit push (set SAVE_MERGED_16BIT=True and PUSH_MERGED_16BIT=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bcb83e",
   "metadata": {},
   "source": [
    "## 13. Inference Test\n",
    "\n",
    "Test the fine-tuned model to verify it outputs raw Cypher queries (matching evaluation pipeline).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19c131",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test with a sample question (use one from your dataset or create new)\n",
    "test_question = \"Which genes are targeted by EGFR inhibitors?\"\n",
    "\n",
    "# Format prompt exactly like evaluation pipeline\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": test_question}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    ")\n",
    "\n",
    "print(\"Formatted prompt (first 500 chars):\")\n",
    "print(text[:500])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806edf8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Generate Cypher query\n",
    "from transformers import TextStreamer\n",
    "\n",
    "print(\"Generated Cypher query:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **tokenizer(text, return_tensors=\"pt\").to(model.device),\n",
    "    max_new_tokens=512,  # Adjust based on expected Cypher query length\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    ")\n",
    "\n",
    "# Decode only new tokens (skip input)\n",
    "input_length = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "generated_tokens = outputs[0][input_length:]\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Clean up markdown code fences if present (matching evaluation pipeline)\n",
    "if \"```\" in generated_text:\n",
    "    lines = generated_text.split(\"\\n\")\n",
    "    cleaned = [line for line in lines if not line.strip().startswith(\"```\")]\n",
    "    generated_text = \"\\n\".join(cleaned).strip()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Final cleaned output (should be raw Cypher only):\")\n",
    "print(\"-\" * 80)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249d3b1",
   "metadata": {},
   "source": [
    "## 14. Download Model Files (Optional)\n",
    "\n",
    "If you want to download the saved models to your local machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea6d44",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Zip and download LoRA adapters\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "try:\n",
    "    shutil.make_archive(LORA_OUTPUT_DIR, 'zip', LORA_OUTPUT_DIR)\n",
    "    files.download(f\"{LORA_OUTPUT_DIR}.zip\")\n",
    "    print(f\"Downloaded {LORA_OUTPUT_DIR}.zip\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating/downloading zip: {e}\")\n",
    "    print(\"You can manually download from Colab file browser\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
